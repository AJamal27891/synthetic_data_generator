{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Validation & Snowflake Load  \n",
    "This notebook validates the synthetic TPC-DS–like dataset generated by our project.  It performs the following checks:  \n",
    "- Ensures all CSVs defined in the lineage config are present and loaded  \n",
    "- Verifies primary key uniqueness for each table  \n",
    "- Checks foreign key consistency (normalizing any `_date` columns)  \n",
    "- Stages the files to Snowflake  \n",
    "- Loads each table into Snowflake using `schema_metadata.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load environment variables, configurations, and build dynamic maps  \n",
    "In this cell we read Snowflake credentials from `.env`, load our table/lineage YAML, and derive the list of CSVs, date columns, PKs, and FK rules entirely from config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Will load CSVs: ['accounts_payable.csv', 'accounts_payable_clean.csv', 'accounts_payable_dw.csv', 'accounts_payable_enriched.csv', 'accounts_receivable.csv', 'accounts_receivable_clean.csv', 'accounts_receivable_dw.csv', 'accounts_receivable_enriched.csv', 'ad_spend.csv', 'ad_spend_clean.csv', 'ad_spend_dw.csv', 'ad_spend_enriched.csv', 'anomaly_detections.csv', 'anomaly_detections_clean.csv', 'anomaly_detections_dw.csv', 'anomaly_detections_enriched.csv', 'asset_management.csv', 'asset_management_clean.csv', 'asset_management_dw.csv', 'asset_management_enriched.csv', 'attendance_records.csv', 'attendance_records_clean.csv', 'attendance_records_dw.csv', 'attendance_records_enriched.csv', 'bank_reconciliations.csv', 'bank_reconciliations_clean.csv', 'bank_reconciliations_dw.csv', 'bank_reconciliations_enriched.csv', 'benefits_enrollment.csv', 'benefits_enrollment_clean.csv', 'benefits_enrollment_dw.csv', 'benefits_enrollment_enriched.csv', 'billing_cycles.csv', 'billing_cycles_clean.csv', 'billing_cycles_dw.csv', 'billing_cycles_enriched.csv', 'brand_partners.csv', 'brand_partners_clean.csv', 'brand_partners_dw.csv', 'brand_partners_enriched.csv', 'budgets_fin.csv', 'budgets_fin_clean.csv', 'budgets_fin_dw.csv', 'budgets_fin_enriched.csv', 'call_records.csv', 'call_records_clean.csv', 'call_records_dw.csv', 'call_records_enriched.csv', 'campaigns.csv', 'campaigns_clean.csv', 'campaigns_dw.csv', 'campaigns_enriched.csv', 'carriers.csv', 'carriers_clean.csv', 'carriers_dw.csv', 'carriers_enriched.csv', 'categories.csv', 'categories_clean.csv', 'categories_dw.csv', 'categories_enriched.csv', 'chat_logs.csv', 'chat_logs_clean.csv', 'chat_logs_dw.csv', 'chat_logs_enriched.csv', 'commissions.csv', 'commissions_clean.csv', 'commissions_dw.csv', 'commissions_enriched.csv', 'compensation_bands.csv', 'compensation_bands_clean.csv', 'compensation_bands_dw.csv', 'compensation_bands_enriched.csv', 'cost_centers.csv', 'cost_centers_clean.csv', 'cost_centers_dw.csv', 'cost_centers_enriched.csv', 'credit_notes.csv', 'credit_notes_clean.csv', 'credit_notes_dw.csv', 'credit_notes_enriched.csv', 'currency_rates.csv', 'currency_rates_clean.csv', 'currency_rates_dw.csv', 'currency_rates_enriched.csv', 'customer_segments.csv', 'customer_segments_clean.csv', 'customer_segments_dw.csv', 'customer_segments_enriched.csv', 'customer_segments_marketing.csv', 'customer_segments_marketing_clean.csv', 'customer_segments_marketing_dw.csv', 'customer_segments_marketing_enriched.csv', 'customers.csv', 'customers_clean.csv', 'customers_dw.csv', 'customers_enriched.csv', 'dashboard_views.csv', 'dashboard_views_clean.csv', 'dashboard_views_dw.csv', 'dashboard_views_enriched.csv', 'data_quality_metrics.csv', 'data_quality_metrics_clean.csv', 'data_quality_metrics_dw.csv', 'data_quality_metrics_enriched.csv', 'dates.csv', 'distribution_centers.csv', 'distribution_centers_clean.csv', 'distribution_centers_dw.csv', 'distribution_centers_enriched.csv', 'email_campaigns.csv', 'email_campaigns_clean.csv', 'email_campaigns_dw.csv', 'email_campaigns_enriched.csv', 'employees.csv', 'employees_clean.csv', 'employees_dw.csv', 'employees_enriched.csv', 'equipment_logs.csv', 'equipment_logs_clean.csv', 'equipment_logs_dw.csv', 'equipment_logs_enriched.csv', 'escalations.csv', 'escalations_clean.csv', 'escalations_dw.csv', 'escalations_enriched.csv', 'event_logs.csv', 'event_logs_clean.csv', 'event_logs_dw.csv', 'event_logs_enriched.csv', 'expense_reports.csv', 'expense_reports_clean.csv', 'expense_reports_dw.csv', 'expense_reports_enriched.csv', 'facility_inspections.csv', 'facility_inspections_clean.csv', 'facility_inspections_dw.csv', 'facility_inspections_enriched.csv', 'faq_hits.csv', 'faq_hits_clean.csv', 'faq_hits_dw.csv', 'faq_hits_enriched.csv', 'financial_statements.csv', 'financial_statements_clean.csv', 'financial_statements_dw.csv', 'financial_statements_enriched.csv', 'inventory_levels.csv', 'inventory_levels_clean.csv', 'inventory_levels_dw.csv', 'inventory_levels_enriched.csv', 'inventory_snapshots.csv', 'inventory_snapshots_clean.csv', 'inventory_snapshots_dw.csv', 'inventory_snapshots_enriched.csv', 'invoices.csv', 'invoices_clean.csv', 'invoices_dw.csv', 'invoices_enriched.csv', 'invoices_fin.csv', 'invoices_fin_clean.csv', 'invoices_fin_dw.csv', 'invoices_fin_enriched.csv', 'job_postings.csv', 'job_postings_clean.csv', 'job_postings_dw.csv', 'job_postings_enriched.csv', 'knowledge_base_articles.csv', 'knowledge_base_articles_clean.csv', 'knowledge_base_articles_dw.csv', 'knowledge_base_articles_enriched.csv', 'kpi_definitions.csv', 'kpi_definitions_clean.csv', 'kpi_definitions_dw.csv', 'kpi_definitions_enriched.csv', 'leads.csv', 'leads_clean.csv', 'leads_dw.csv', 'leads_enriched.csv', 'ledgers.csv', 'ledgers_clean.csv', 'ledgers_dw.csv', 'ledgers_enriched.csv', 'logistics_routes.csv', 'logistics_routes_clean.csv', 'logistics_routes_dw.csv', 'logistics_routes_enriched.csv', 'loyalty_programs.csv', 'loyalty_programs_clean.csv', 'loyalty_programs_dw.csv', 'loyalty_programs_enriched.csv', 'maintenance_requests.csv', 'maintenance_requests_clean.csv', 'maintenance_requests_dw.csv', 'maintenance_requests_enriched.csv', 'marketing_targets.csv', 'marketing_targets_clean.csv', 'marketing_targets_dw.csv', 'marketing_targets_enriched.csv', 'opportunities.csv', 'opportunities_clean.csv', 'opportunities_dw.csv', 'opportunities_enriched.csv', 'order_items.csv', 'order_items_clean.csv', 'order_items_dw.csv', 'order_items_enriched.csv', 'orders.csv', 'orders_backup.csv', 'orders_clean.csv', 'orders_dw.csv', 'orders_enriched.csv', 'org_chart.csv', 'org_chart_clean.csv', 'org_chart_dw.csv', 'org_chart_enriched.csv', 'packaging.csv', 'packaging_clean.csv', 'packaging_dw.csv', 'packaging_enriched.csv', 'payments.csv', 'payments_clean.csv', 'payments_dw.csv', 'payments_enriched.csv', 'payments_fin.csv', 'payments_fin_clean.csv', 'payments_fin_dw.csv', 'payments_fin_enriched.csv', 'payroll.csv', 'payroll_clean.csv', 'payroll_dw.csv', 'payroll_enriched.csv', 'performance_reviews.csv', 'performance_reviews_clean.csv', 'performance_reviews_dw.csv', 'performance_reviews_enriched.csv', 'predictive_models.csv', 'predictive_models_clean.csv', 'predictive_models_dw.csv', 'predictive_models_enriched.csv', 'pricing_tiers.csv', 'pricing_tiers_clean.csv', 'pricing_tiers_dw.csv', 'pricing_tiers_enriched.csv', 'product_attributes.csv', 'product_attributes_clean.csv', 'product_attributes_dw.csv', 'product_attributes_enriched.csv', 'product_images.csv', 'product_images_clean.csv', 'product_images_dw.csv', 'product_images_enriched.csv', 'product_reviews.csv', 'product_reviews_clean.csv', 'product_reviews_dw.csv', 'product_reviews_enriched.csv', 'products.csv', 'products_clean.csv', 'products_dw.csv', 'products_enriched.csv', 'profit_centers.csv', 'profit_centers_clean.csv', 'profit_centers_dw.csv', 'profit_centers_enriched.csv', 'recruitment_applications.csv', 'recruitment_applications_clean.csv', 'recruitment_applications_dw.csv', 'recruitment_applications_enriched.csv', 'refunds.csv', 'refunds_clean.csv', 'refunds_dw.csv', 'refunds_enriched.csv', 'related_products.csv', 'related_products_clean.csv', 'related_products_dw.csv', 'related_products_enriched.csv', 'report_configs.csv', 'report_configs_clean.csv', 'report_configs_dw.csv', 'report_configs_enriched.csv', 'resolution_times.csv', 'resolution_times_clean.csv', 'resolution_times_dw.csv', 'resolution_times_enriched.csv', 'sales_channels.csv', 'sales_channels_clean.csv', 'sales_channels_dw.csv', 'sales_channels_enriched.csv', 'sales_forecast.csv', 'sales_forecast_clean.csv', 'sales_forecast_dw.csv', 'sales_forecast_enriched.csv', 'sales_pipeline.csv', 'sales_pipeline_clean.csv', 'sales_pipeline_dw.csv', 'sales_pipeline_enriched.csv', 'sales_regions.csv', 'sales_regions_clean.csv', 'sales_regions_dw.csv', 'sales_regions_enriched.csv', 'sales_targets.csv', 'sales_targets_clean.csv', 'sales_targets_dw.csv', 'sales_targets_enriched.csv', 'satisfaction_surveys.csv', 'satisfaction_surveys_clean.csv', 'satisfaction_surveys_dw.csv', 'satisfaction_surveys_enriched.csv', 'schedule_shifts.csv', 'schedule_shifts_clean.csv', 'schedule_shifts_dw.csv', 'schedule_shifts_enriched.csv', 'segmentation_results.csv', 'segmentation_results_clean.csv', 'segmentation_results_dw.csv', 'segmentation_results_enriched.csv', 'seo_keywords.csv', 'seo_keywords_clean.csv', 'seo_keywords_dw.csv', 'seo_keywords_enriched.csv', 'session_data.csv', 'session_data_clean.csv', 'session_data_dw.csv', 'session_data_enriched.csv', 'shipments.csv', 'shipments_clean.csv', 'shipments_dw.csv', 'shipments_enriched.csv', 'social_media_metrics.csv', 'social_media_metrics_clean.csv', 'social_media_metrics_dw.csv', 'social_media_metrics_enriched.csv', 'store_staffing.csv', 'store_staffing_clean.csv', 'store_staffing_dw.csv', 'store_staffing_enriched.csv', 'stores.csv', 'stores_clean.csv', 'stores_dw.csv', 'stores_enriched.csv', 'subcategories.csv', 'subcategories_clean.csv', 'subcategories_dw.csv', 'subcategories_enriched.csv', 'support_agents.csv', 'support_agents_clean.csv', 'support_agents_dw.csv', 'support_agents_enriched.csv', 'survey_responses.csv', 'survey_responses_clean.csv', 'survey_responses_dw.csv', 'survey_responses_enriched.csv', 'tax_records.csv', 'tax_records_clean.csv', 'tax_records_dw.csv', 'tax_records_enriched.csv', 'ticket_responses.csv', 'ticket_responses_clean.csv', 'ticket_responses_dw.csv', 'ticket_responses_enriched.csv', 'tickets.csv', 'tickets_clean.csv', 'tickets_dw.csv', 'tickets_enriched.csv', 'training_sessions.csv', 'training_sessions_clean.csv', 'training_sessions_dw.csv', 'training_sessions_enriched.csv', 'transactions_backup.csv', 'transactions_dw.csv', 'transfer_orders.csv', 'transfer_orders_clean.csv', 'transfer_orders_dw.csv', 'transfer_orders_enriched.csv', 'trend_analysis.csv', 'trend_analysis_clean.csv', 'trend_analysis_dw.csv', 'trend_analysis_enriched.csv', 'warehouse_locations.csv', 'warehouse_locations_clean.csv', 'warehouse_locations_dw.csv', 'warehouse_locations_enriched.csv', 'web_traffic.csv', 'web_traffic_clean.csv', 'web_traffic_dw.csv', 'web_traffic_enriched.csv']\n",
      "→ Date parsing map: {'dates': ['full_date', 'day', 'month', 'year', 'weekday'], 'orders': ['order_date'], 'invoices_finance': ['invoice_date'], 'tax_records': ['tax_date'], 'bank_reconciliations': ['date'], 'payments_legacy': ['paid_date'], 'campaigns': ['start_date', 'end_date'], 'ad_spend': ['date'], 'social_media_metrics': ['date'], 'email_campaigns': ['sent_date'], 'loyalty_programs': ['start_date'], 'survey_responses': ['response_date'], 'employees': ['hire_date'], 'attendance_records': ['date'], 'payroll': ['pay_date'], 'benefits_enrollment': ['start_date'], 'performance_reviews': ['review_date'], 'training_sessions': ['session_date'], 'recruitment_applications': ['application_date'], 'job_postings': ['post_date'], 'escalations': ['date'], 'knowledge_base_articles': ['created_at'], 'faq_hits': ['hit_date']}\n",
      "→ Primary keys: {'customers': 'customer_id', 'products': 'product_id', 'stores': 'store_id', 'distribution_centers': 'center_id', 'categories': 'category_id', 'subcategories': 'subcategory_id', 'product_reviews': 'review_id', 'product_images': 'image_id', 'pricing_tiers': 'tier_id', 'inventory_levels': 'level_id', 'product_attributes': 'attribute_id', 'brand_partners': 'partner_id', 'dates': 'date_id', 'budgets': 'budget_id', 'ledgers': 'ledger_id', 'orders': 'order_id', 'warehouse_locations': 'warehouse_id', 'invoices_finance': 'invoice_id', 'tax_records': 'record_id', 'financial_statements': 'statement_id', 'accounts_receivable': 'ar_id', 'accounts_payable': 'ap_id', 'cost_centers': 'center_id', 'profit_centers': 'center_id', 'currency_rates': 'rate_id', 'bank_reconciliations': 'reconciliation_id', 'credit_notes': 'credit_note_id', 'payments_legacy': 'payment_id', 'campaigns': 'campaign_id', 'ad_spend': 'spend_id', 'social_media_metrics': 'metric_id', 'seo_keywords': 'keyword_id', 'email_campaigns': 'email_id', 'web_traffic': 'visit_id', 'loyalty_programs': 'program_id', 'survey_responses': 'response_id', 'marketing_targets': 'target_id', 'customer_segments_marketing': 'segment_id', 'employees': 'employee_id', 'attendance_records': 'record_id', 'payroll': 'payroll_id', 'benefits_enrollment': 'enrollment_id', 'performance_reviews': 'review_id', 'training_sessions': 'session_id', 'recruitment_applications': 'application_id', 'job_postings': 'posting_id', 'org_chart': 'node_id', 'compensation_bands': 'band_id', 'dashboard_views': 'view_id', 'kpi_definitions': 'kpi_id', 'event_logs': 'event_id', 'session_data': 'session_id', 'data_quality_metrics': 'metric_id', 'anomaly_detections': 'detection_id', 'predictive_models': 'model_id', 'segmentation_results': 'segment_id', 'trend_analysis': 'analysis_id', 'report_configs': 'report_id', 'tickets': 'ticket_id', 'ticket_responses': 'response_id', 'chat_logs': 'chat_id', 'call_records': 'call_id', 'satisfaction_surveys': 'survey_id', 'escalations': 'escalation_id', 'knowledge_base_articles': 'article_id', 'support_agents': 'agent_id', 'faq_hits': 'hit_id', 'resolution_times': 'record_id'}\n",
      "→ Foreign key rules: [{'table': 'orders', 'column': 'customer_id', 'references': 'customers.customer_id'}, {'table': 'orders', 'column': 'store_id', 'references': 'stores.store_id'}, {'table': 'order_items', 'column': 'order_id', 'references': 'orders.order_id'}, {'table': 'order_items', 'column': 'product_id', 'references': 'products.product_id'}, {'table': 'invoices', 'column': 'order_id', 'references': 'orders.order_id'}, {'table': 'payments', 'column': 'invoice_id', 'references': 'invoices.invoice_id'}, {'table': 'refunds', 'column': 'payment_id', 'references': 'payments.payment_id'}, {'table': 'sales_pipeline', 'column': 'customer_id', 'references': 'customers.customer_id'}, {'table': 'sales_pipeline', 'column': 'opportunity_id', 'references': 'opportunities.opportunity_id'}, {'table': 'opportunities', 'column': 'lead_id', 'references': 'leads.lead_id'}, {'table': 'commissions', 'column': 'opportunity_id', 'references': 'opportunities.opportunity_id'}, {'table': 'sales_targets', 'column': 'sales_forecast_id', 'references': 'sales_forecast.forecast_id'}, {'table': 'inventory_levels', 'column': 'product_id', 'references': 'products.product_id'}, {'table': 'inventory_levels', 'column': 'warehouse_id', 'references': 'warehouse_locations.warehouse_id'}, {'table': 'inventory_snapshots', 'column': 'product_id', 'references': 'products.product_id'}, {'table': 'inventory_snapshots', 'column': 'warehouse_id', 'references': 'warehouse_locations.warehouse_id'}, {'table': 'shipments', 'column': 'order_item_id', 'references': 'order_items.item_id'}, {'table': 'shipments', 'column': 'carrier_id', 'references': 'carriers.carrier_id'}, {'table': 'maintenance_requests', 'column': 'equipment_id', 'references': 'equipment_logs.equipment_id'}, {'table': 'store_staffing', 'column': 'store_id', 'references': 'stores.store_id'}, {'table': 'store_staffing', 'column': 'employee_id', 'references': 'employees.employee_id'}, {'table': 'schedule_shifts', 'column': 'store_id', 'references': 'stores.store_id'}, {'table': 'schedule_shifts', 'column': 'employee_id', 'references': 'employees.employee_id'}, {'table': 'facility_inspections', 'column': 'store_id', 'references': 'stores.store_id'}, {'table': 'equipment_logs', 'column': 'asset_id', 'references': 'asset_management.asset_id'}, {'table': 'transfer_orders', 'column': 'from_center', 'references': 'distribution_centers.center_id'}, {'table': 'transfer_orders', 'column': 'to_center', 'references': 'distribution_centers.center_id'}, {'table': 'packaging', 'column': 'shipment_id', 'references': 'shipments.shipment_id'}, {'table': 'invoices_finance', 'column': 'billing_cycle_id', 'references': 'billing_cycles.billing_cycle_id'}, {'table': 'payments_finance', 'column': 'invoice_id', 'references': 'invoices_finance.invoice_id'}, {'table': 'expense_reports', 'column': 'employee_id', 'references': 'employees.employee_id'}, {'table': 'tax_records', 'column': 'invoice_id', 'references': 'invoices_finance.invoice_id'}, {'table': 'accounts_receivable', 'column': 'customer_id', 'references': 'customers.customer_id'}, {'table': 'credit_notes', 'column': 'invoice_id', 'references': 'invoices_finance.invoice_id'}, {'table': 'ad_spend', 'column': 'campaign_id', 'references': 'campaigns.campaign_id'}, {'table': 'email_campaigns', 'column': 'campaign_id', 'references': 'campaigns.campaign_id'}, {'table': 'marketing_targets', 'column': 'campaign_id', 'references': 'campaigns.campaign_id'}, {'table': 'attendance_records', 'column': 'employee_id', 'references': 'employees.employee_id'}, {'table': 'payroll', 'column': 'employee_id', 'references': 'employees.employee_id'}, {'table': 'benefits_enrollment', 'column': 'employee_id', 'references': 'employees.employee_id'}, {'table': 'performance_reviews', 'column': 'employee_id', 'references': 'employees.employee_id'}, {'table': 'org_chart', 'column': 'manager_id', 'references': 'employees.employee_id'}, {'table': 'org_chart', 'column': 'subordinate_id', 'references': 'employees.employee_id'}, {'table': 'anomaly_detections', 'column': 'metric_id', 'references': 'data_quality_metrics.metric_id'}, {'table': 'segmentation_results', 'column': 'kpi_id', 'references': 'kpi_definitions.kpi_id'}, {'table': 'trend_analysis', 'column': 'metric_id', 'references': 'data_quality_metrics.metric_id'}, {'table': 'tickets', 'column': 'customer_id', 'references': 'customers.customer_id'}, {'table': 'ticket_responses', 'column': 'ticket_id', 'references': 'tickets.ticket_id'}, {'table': 'chat_logs', 'column': 'customer_id', 'references': 'customers.customer_id'}, {'table': 'chat_logs', 'column': 'agent_id', 'references': 'support_agents.agent_id'}, {'table': 'call_records', 'column': 'ticket_id', 'references': 'tickets.ticket_id'}, {'table': 'satisfaction_surveys', 'column': 'ticket_id', 'references': 'tickets.ticket_id'}, {'table': 'escalations', 'column': 'ticket_id', 'references': 'tickets.ticket_id'}, {'table': 'faq_hits', 'column': 'article_id', 'references': 'knowledge_base_articles.article_id'}, {'table': 'resolution_times', 'column': 'ticket_id', 'references': 'tickets.ticket_id'}]\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "import os, json, yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ——————————————————————————————\n",
    "# Setup paths & env\n",
    "# ——————————————————————————————\n",
    "ROOT_DIR = Path.cwd().parent             # one level up from notebooks/\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "META_DIR = ROOT_DIR / \"metadata\"\n",
    "CFG_DIR  = ROOT_DIR / \"data_generation_config\"\n",
    "load_dotenv(dotenv_path=ROOT_DIR / \".env\")\n",
    "\n",
    "# ——————————————————————————————\n",
    "# Load YAML configs\n",
    "# ——————————————————————————————\n",
    "with open(CFG_DIR / \"tables.yml\")  as f: tbl_cfg = yaml.safe_load(f)\n",
    "with open(CFG_DIR / \"lineage.yml\") as f: lin_cfg = yaml.safe_load(f)\n",
    "\n",
    "# ——————————————————————————————\n",
    "# 1.a) Build list of CSVs from lineage steps\n",
    "# ——————————————————————————————\n",
    "file_set = set()\n",
    "for step in lin_cfg[\"steps\"]:\n",
    "    # Handle inputs\n",
    "    for input_file in step.get(\"inputs\", []):\n",
    "        if isinstance(input_file, list):\n",
    "            # If input_file is a list, add each item\n",
    "            for nested_file in input_file:\n",
    "                file_set.add(nested_file)\n",
    "        else:\n",
    "            # If input_file is a string, add it directly\n",
    "            file_set.add(input_file)\n",
    "    \n",
    "    # Handle outputs similarly\n",
    "    for output_file in step.get(\"outputs\", []):\n",
    "        if isinstance(output_file, list):\n",
    "            for nested_file in output_file:\n",
    "                file_set.add(nested_file)\n",
    "        else:\n",
    "            file_set.add(output_file)\n",
    "\n",
    "# include any backups if present on disk\n",
    "for p in DATA_DIR.glob(\"*_backup.csv\"):\n",
    "    file_set.add(p.name)\n",
    "file_list = sorted(file_set)\n",
    "\n",
    "# ——————————————————————————————\n",
    "# 1.b) Derive date columns from tables.yml\n",
    "# ——————————————————————————————\n",
    "date_map = {}\n",
    "from datetime import timedelta, datetime\n",
    "for tname, tdef in tbl_cfg[\"tables\"].items():\n",
    "    dates = []\n",
    "    for col, cdef in tdef[\"columns\"].items():\n",
    "        # faker.date_between or derived expr containing 'date'\n",
    "        if (cdef[\"type\"] == \"faker\"       and cdef.get(\"method\")==\"date_between\") \\\n",
    "        or (cdef[\"type\"] == \"derived\"     and \"date\" in cdef.get(\"expr\",\"\")):\n",
    "            dates.append(col)\n",
    "    if dates:\n",
    "        date_map[tname] = dates\n",
    "\n",
    "# 1.c) Derive PKs by convention: first int _id\n",
    "PKS = {}\n",
    "for tname, tdef in tbl_cfg[\"tables\"].items():\n",
    "    for col, cdef in tdef[\"columns\"].items():\n",
    "        if col.endswith(\"_id\") and cdef[\"type\"]==\"int\":\n",
    "            PKS[tname] = col\n",
    "            break\n",
    "\n",
    "# 1.d) Load schema_metadata for FK definitions\n",
    "schema_meta   = json.load(open(META_DIR/\"schema_metadata.json\"))\n",
    "relationships = schema_meta.get(\"relationships\", [])\n",
    "\n",
    "print(\"→ Will load CSVs:\", file_list)\n",
    "print(\"→ Date parsing map:\", date_map)\n",
    "print(\"→ Primary keys:\", PKS)\n",
    "print(\"→ Foreign key rules:\", relationships)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_list)\n",
    "len(date_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== DATA INTEGRITY AUDIT REPORT =====\n",
      "\n",
      "✓ No integrity issues found!\n"
     ]
    }
   ],
   "source": [
    "def audit_data_schema_integrity():\n",
    "    \"\"\"\n",
    "    Audits the generated data files and schema_metadata.json for integrity issues\n",
    "    that could cause validation failures or Snowflake load errors.\n",
    "    \"\"\"\n",
    "    import os, json, yaml\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Set up paths\n",
    "    ROOT_DIR = Path.cwd().parent\n",
    "    DATA_DIR = ROOT_DIR / \"data\"\n",
    "    META_DIR = ROOT_DIR / \"metadata\"\n",
    "    schema_path = META_DIR / \"schema_metadata.json\"\n",
    "    \n",
    "    # Load schema metadata\n",
    "    with open(schema_path, 'r') as f:\n",
    "        schema = json.load(f)\n",
    "    \n",
    "    issues = {\n",
    "        \"type_mismatches\": [],\n",
    "        \"malformed_references\": [],\n",
    "        \"missing_columns\": [],\n",
    "        \"missing_tables\": [],\n",
    "        \"fk_violations\": []\n",
    "    }\n",
    "    \n",
    "    # 1. Check for malformed references in relationships\n",
    "    for idx, rel in enumerate(schema.get(\"relationships\", [])):\n",
    "        if \"references\" not in rel or \".\" not in str(rel.get(\"references\", \"\")):\n",
    "            issues[\"malformed_references\"].append({\n",
    "                \"index\": idx,\n",
    "                \"relation\": rel\n",
    "            })\n",
    "            continue\n",
    "            \n",
    "        # For valid references, check if tables and columns exist\n",
    "        child_table, child_col = rel.get(\"table\", \"\"), rel.get(\"column\", \"\")\n",
    "        parent_ref = rel.get(\"references\", \"\")\n",
    "        \n",
    "        if \".\" in parent_ref:\n",
    "            parent_table, parent_col = parent_ref.split(\".\", 1)\n",
    "            \n",
    "            # Check if tables exist\n",
    "            child_path = DATA_DIR / f\"{child_table}.csv\"\n",
    "            parent_path = DATA_DIR / f\"{parent_table}.csv\"\n",
    "            \n",
    "            if not child_path.exists():\n",
    "                issues[\"missing_tables\"].append(f\"Child table {child_table} not found\")\n",
    "            if not parent_path.exists():\n",
    "                issues[\"missing_tables\"].append(f\"Parent table {parent_table} not found\")\n",
    "            \n",
    "            # If tables exist, check if columns exist\n",
    "            if child_path.exists():\n",
    "                child_df = pd.read_csv(child_path, nrows=0)\n",
    "                if child_col not in child_df.columns:\n",
    "                    issues[\"missing_columns\"].append(f\"{child_table}.{child_col}\")\n",
    "            \n",
    "            if parent_path.exists():\n",
    "                parent_df = pd.read_csv(parent_path, nrows=0)\n",
    "                if parent_col not in parent_df.columns:\n",
    "                    issues[\"missing_columns\"].append(f\"{parent_table}.{parent_col}\")\n",
    "    \n",
    "    # 2. Check for type mismatches\n",
    "    for table_name, table_meta in schema.get(\"tables\", {}).items():\n",
    "        table_path = DATA_DIR / f\"{table_name}.csv\"\n",
    "        if not table_path.exists():\n",
    "            continue\n",
    "            \n",
    "        # Read the actual CSV\n",
    "        df = pd.read_csv(table_path)\n",
    "        \n",
    "        # Check each column's type against schema definition\n",
    "        for col_name, col_meta in table_meta.get(\"columns\", {}).items():\n",
    "            if col_name not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            # Get expected type from schema\n",
    "            if isinstance(col_meta, dict):\n",
    "                expected_type = col_meta.get(\"type\", \"\").upper()\n",
    "            else:\n",
    "                expected_type = str(col_meta).upper()\n",
    "            \n",
    "            # Check for numeric columns with non-numeric values\n",
    "            if any(t in expected_type for t in [\"INT\", \"FLOAT\", \"DECIMAL\", \"NUMERIC\", \"DOUBLE\"]):\n",
    "                try:\n",
    "                    pd.to_numeric(df[col_name])\n",
    "                except:\n",
    "                    # Found non-numeric data in numeric column\n",
    "                    # Get sample of problematic values\n",
    "                    non_numeric = []\n",
    "                    for val in df[col_name].dropna().unique():\n",
    "                        try:\n",
    "                            float(val)\n",
    "                        except:\n",
    "                            non_numeric.append(str(val))\n",
    "                            if len(non_numeric) >= 3:\n",
    "                                break\n",
    "                    \n",
    "                    issues[\"type_mismatches\"].append({\n",
    "                        \"table\": table_name,\n",
    "                        \"column\": col_name,\n",
    "                        \"expected_type\": expected_type,\n",
    "                        \"sample_values\": non_numeric[:3]\n",
    "                    })\n",
    "    \n",
    "    # 3. Print detailed report\n",
    "    print(\"\\n===== DATA INTEGRITY AUDIT REPORT =====\\n\")\n",
    "    \n",
    "    if issues[\"malformed_references\"]:\n",
    "        print(f\"MALFORMED REFERENCES ({len(issues['malformed_references'])} issues):\")\n",
    "        for issue in issues[\"malformed_references\"]:\n",
    "            print(f\"  • Relation #{issue['index']}: {issue['relation']}\")\n",
    "        print()\n",
    "    \n",
    "    if issues[\"missing_tables\"]:\n",
    "        print(f\"MISSING TABLES ({len(issues['missing_tables'])} issues):\")\n",
    "        for table in sorted(set(issues[\"missing_tables\"])):\n",
    "            print(f\"  • {table}\")\n",
    "        print()\n",
    "    \n",
    "    if issues[\"missing_columns\"]:\n",
    "        print(f\"MISSING COLUMNS ({len(issues['missing_columns'])} issues):\")\n",
    "        for col in sorted(set(issues[\"missing_columns\"])):\n",
    "            print(f\"  • {col}\")\n",
    "        print()\n",
    "    \n",
    "    if issues[\"type_mismatches\"]:\n",
    "        print(f\"TYPE MISMATCHES ({len(issues['type_mismatches'])} issues):\")\n",
    "        for issue in issues[\"type_mismatches\"]:\n",
    "            print(f\"  • {issue['table']}.{issue['column']} - Expected {issue['expected_type']}, \" \n",
    "                  f\"found non-numeric values: {', '.join(issue['sample_values'])}\")\n",
    "        print()\n",
    "    \n",
    "    if not any(issues.values()):\n",
    "        print(\"✓ No integrity issues found!\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Run the audit\n",
    "audit_results = audit_data_schema_integrity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load CSVs with proper date parsing  \n",
    "Here we load each CSV present on disk, automatically parsing only those columns whose names end in `_date`, and skip any files declared in lineage but not generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSVs…\n",
      " • accounts_payable     → 15234 rows ×  3 cols\n",
      " • accounts_payable_clean MISSING on disk, skipping\n",
      " • accounts_payable_dw  MISSING on disk, skipping\n",
      " • accounts_payable_enriched MISSING on disk, skipping\n",
      " • accounts_receivable  → 16890 rows ×  3 cols\n",
      " • accounts_receivable_clean MISSING on disk, skipping\n",
      " • accounts_receivable_dw MISSING on disk, skipping\n",
      " • accounts_receivable_enriched MISSING on disk, skipping\n",
      " • ad_spend             → 15678 rows ×  4 cols\n",
      " • ad_spend_clean       MISSING on disk, skipping\n",
      " • ad_spend_dw          MISSING on disk, skipping\n",
      " • ad_spend_enriched    MISSING on disk, skipping\n",
      " • anomaly_detections   → 43210 rows ×  3 cols\n",
      " • anomaly_detections_clean MISSING on disk, skipping\n",
      " • anomaly_detections_dw MISSING on disk, skipping\n",
      " • anomaly_detections_enriched MISSING on disk, skipping\n",
      " • asset_management     →  2765 rows ×  4 cols  (dates: ['purchase_date'])\n",
      " • asset_management_clean MISSING on disk, skipping\n",
      " • asset_management_dw  MISSING on disk, skipping\n",
      " • asset_management_enriched MISSING on disk, skipping\n",
      " • attendance_records   → 18987 rows ×  4 cols\n",
      " • attendance_records_clean MISSING on disk, skipping\n",
      " • attendance_records_dw MISSING on disk, skipping\n",
      " • attendance_records_enriched MISSING on disk, skipping\n",
      " • bank_reconciliations → 14532 rows ×  2 cols\n",
      " • bank_reconciliations_clean MISSING on disk, skipping\n",
      " • bank_reconciliations_dw MISSING on disk, skipping\n",
      " • bank_reconciliations_enriched MISSING on disk, skipping\n",
      " • benefits_enrollment  →  8456 rows ×  4 cols  (dates: ['start_date'])\n",
      " • benefits_enrollment_clean MISSING on disk, skipping\n",
      " • benefits_enrollment_dw MISSING on disk, skipping\n",
      " • benefits_enrollment_enriched MISSING on disk, skipping\n",
      " • billing_cycles       → 19876 rows ×  3 cols  (dates: ['start_date', 'end_date'])\n",
      " • billing_cycles_clean MISSING on disk, skipping\n",
      " • billing_cycles_dw    MISSING on disk, skipping\n",
      " • billing_cycles_enriched MISSING on disk, skipping\n",
      " • brand_partners       →  1345 rows ×  3 cols\n",
      " • brand_partners_clean MISSING on disk, skipping\n",
      " • brand_partners_dw    MISSING on disk, skipping\n",
      " • brand_partners_enriched MISSING on disk, skipping\n",
      " • budgets_fin          MISSING on disk, skipping\n",
      " • budgets_fin_clean    MISSING on disk, skipping\n",
      " • budgets_fin_dw       MISSING on disk, skipping\n",
      " • budgets_fin_enriched MISSING on disk, skipping\n",
      " • call_records         → 12098 rows ×  4 cols\n",
      " • call_records_clean   MISSING on disk, skipping\n",
      " • call_records_dw      MISSING on disk, skipping\n",
      " • call_records_enriched MISSING on disk, skipping\n",
      " • campaigns            → 13245 rows ×  5 cols  (dates: ['start_date', 'end_date'])\n",
      " • campaigns_clean      MISSING on disk, skipping\n",
      " • campaigns_dw         MISSING on disk, skipping\n",
      " • campaigns_enriched   MISSING on disk, skipping\n",
      " • carriers             →  2345 rows ×  3 cols\n",
      " • carriers_clean       MISSING on disk, skipping\n",
      " • carriers_dw          MISSING on disk, skipping\n",
      " • carriers_enriched    MISSING on disk, skipping\n",
      " • categories           →  2345 rows ×  3 cols\n",
      " • categories_clean     MISSING on disk, skipping\n",
      " • categories_dw        MISSING on disk, skipping\n",
      " • categories_enriched  MISSING on disk, skipping\n",
      " • chat_logs            → 23109 rows ×  5 cols\n",
      " • chat_logs_clean      MISSING on disk, skipping\n",
      " • chat_logs_dw         MISSING on disk, skipping\n",
      " • chat_logs_enriched   MISSING on disk, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " • commissions          → 29415 rows ×  4 cols\n",
      " • commissions_clean    MISSING on disk, skipping\n",
      " • commissions_dw       MISSING on disk, skipping\n",
      " • commissions_enriched MISSING on disk, skipping\n",
      " • compensation_bands   →  2109 rows ×  4 cols\n",
      " • compensation_bands_clean MISSING on disk, skipping\n",
      " • compensation_bands_dw MISSING on disk, skipping\n",
      " • compensation_bands_enriched MISSING on disk, skipping\n",
      " • cost_centers         → 13456 rows ×  2 cols\n",
      " • cost_centers_clean   MISSING on disk, skipping\n",
      " • cost_centers_dw      MISSING on disk, skipping\n",
      " • cost_centers_enriched MISSING on disk, skipping\n",
      " • credit_notes         → 13245 rows ×  3 cols\n",
      " • credit_notes_clean   MISSING on disk, skipping\n",
      " • credit_notes_dw      MISSING on disk, skipping\n",
      " • credit_notes_enriched MISSING on disk, skipping\n",
      " • currency_rates       → 16789 rows ×  3 cols\n",
      " • currency_rates_clean MISSING on disk, skipping\n",
      " • currency_rates_dw    MISSING on disk, skipping\n",
      " • currency_rates_enriched MISSING on disk, skipping\n",
      " • customer_segments    → 15897 rows ×  3 cols\n",
      " • customer_segments_clean MISSING on disk, skipping\n",
      " • customer_segments_dw MISSING on disk, skipping\n",
      " • customer_segments_enriched MISSING on disk, skipping\n",
      " • customer_segments_marketing → 13459 rows ×  3 cols\n",
      " • customer_segments_marketing_clean MISSING on disk, skipping\n",
      " • customer_segments_marketing_dw MISSING on disk, skipping\n",
      " • customer_segments_marketing_enriched MISSING on disk, skipping\n",
      " • customers            → 59229 rows ×  8 cols\n",
      " • customers_clean      MISSING on disk, skipping\n",
      " • customers_dw         MISSING on disk, skipping\n",
      " • customers_enriched   MISSING on disk, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " • dashboard_views      → 98765 rows ×  4 cols  (dates: ['view_date'])\n",
      " • dashboard_views_clean MISSING on disk, skipping\n",
      " • dashboard_views_dw   MISSING on disk, skipping\n",
      " • dashboard_views_enriched MISSING on disk, skipping\n",
      " • data_quality_metrics → 54321 rows ×  3 cols\n",
      " • data_quality_metrics_clean MISSING on disk, skipping\n",
      " • data_quality_metrics_dw MISSING on disk, skipping\n",
      " • data_quality_metrics_enriched MISSING on disk, skipping\n",
      " • dates                →   365 rows ×  6 cols  (dates: ['full_date'])\n",
      " • distribution_centers →  1456 rows ×  4 cols\n",
      " • distribution_centers_clean MISSING on disk, skipping\n",
      " • distribution_centers_dw MISSING on disk, skipping\n",
      " • distribution_centers_enriched MISSING on disk, skipping\n",
      " • email_campaigns      → 19876 rows ×  4 cols  (dates: ['sent_date'])\n",
      " • email_campaigns_clean MISSING on disk, skipping\n",
      " • email_campaigns_dw   MISSING on disk, skipping\n",
      " • email_campaigns_enriched MISSING on disk, skipping\n",
      " • employees            →   200 rows ×  5 cols  (dates: ['hire_date'])\n",
      " • employees_clean      MISSING on disk, skipping\n",
      " • employees_dw         MISSING on disk, skipping\n",
      " • employees_enriched   MISSING on disk, skipping\n",
      " • equipment_logs       →  1984 rows ×  4 cols  (dates: ['log_date'])\n",
      " • equipment_logs_clean MISSING on disk, skipping\n",
      " • equipment_logs_dw    MISSING on disk, skipping\n",
      " • equipment_logs_enriched MISSING on disk, skipping\n",
      " • escalations          →  9876 rows ×  4 cols\n",
      " • escalations_clean    MISSING on disk, skipping\n",
      " • escalations_dw       MISSING on disk, skipping\n",
      " • escalations_enriched MISSING on disk, skipping\n",
      " • event_logs           → 76543 rows ×  4 cols\n",
      " • event_logs_clean     MISSING on disk, skipping\n",
      " • event_logs_dw        MISSING on disk, skipping\n",
      " • event_logs_enriched  MISSING on disk, skipping\n",
      " • expense_reports      → 14567 rows ×  4 cols  (dates: ['report_date'])\n",
      " • expense_reports_clean MISSING on disk, skipping\n",
      " • expense_reports_dw   MISSING on disk, skipping\n",
      " • expense_reports_enriched MISSING on disk, skipping\n",
      " • facility_inspections →  1543 rows ×  4 cols  (dates: ['inspection_date'])\n",
      " • facility_inspections_clean MISSING on disk, skipping\n",
      " • facility_inspections_dw MISSING on disk, skipping\n",
      " • facility_inspections_enriched MISSING on disk, skipping\n",
      " • faq_hits             →  6543 rows ×  3 cols  (dates: ['hit_date'])\n",
      " • faq_hits_clean       MISSING on disk, skipping\n",
      " • faq_hits_dw          MISSING on disk, skipping\n",
      " • faq_hits_enriched    MISSING on disk, skipping\n",
      " • financial_statements → 14234 rows ×  4 cols\n",
      " • financial_statements_clean MISSING on disk, skipping\n",
      " • financial_statements_dw MISSING on disk, skipping\n",
      " • financial_statements_enriched MISSING on disk, skipping\n",
      " • inventory_levels     →  5678 rows ×  4 cols\n",
      " • inventory_levels_clean MISSING on disk, skipping\n",
      " • inventory_levels_dw  MISSING on disk, skipping\n",
      " • inventory_levels_enriched MISSING on disk, skipping\n",
      " • inventory_snapshots  →  3245 rows ×  5 cols  (dates: ['snapshot_date'])\n",
      " • inventory_snapshots_clean MISSING on disk, skipping\n",
      " • inventory_snapshots_dw MISSING on disk, skipping\n",
      " • inventory_snapshots_enriched MISSING on disk, skipping\n",
      " • invoices             → 45678 rows ×  5 cols  (dates: ['invoice_date'])\n",
      " • invoices_clean       MISSING on disk, skipping\n",
      " • invoices_dw          MISSING on disk, skipping\n",
      " • invoices_enriched    MISSING on disk, skipping\n",
      " • invoices_fin         MISSING on disk, skipping\n",
      " • invoices_fin_clean   MISSING on disk, skipping\n",
      " • invoices_fin_dw      MISSING on disk, skipping\n",
      " • invoices_fin_enriched MISSING on disk, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " • job_postings         →  4321 rows ×  4 cols  (dates: ['post_date'])\n",
      " • job_postings_clean   MISSING on disk, skipping\n",
      " • job_postings_dw      MISSING on disk, skipping\n",
      " • job_postings_enriched MISSING on disk, skipping\n",
      " • knowledge_base_articles →  8765 rows ×  4 cols\n",
      " • knowledge_base_articles_clean MISSING on disk, skipping\n",
      " • knowledge_base_articles_dw MISSING on disk, skipping\n",
      " • knowledge_base_articles_enriched MISSING on disk, skipping\n",
      " • kpi_definitions      → 87654 rows ×  3 cols\n",
      " • kpi_definitions_clean MISSING on disk, skipping\n",
      " • kpi_definitions_dw   MISSING on disk, skipping\n",
      " • kpi_definitions_enriched MISSING on disk, skipping\n",
      " • leads                → 27109 rows ×  4 cols\n",
      " • leads_clean          MISSING on disk, skipping\n",
      " • leads_dw             MISSING on disk, skipping\n",
      " • leads_enriched       MISSING on disk, skipping\n",
      " • ledgers              → 18765 rows ×  3 cols\n",
      " • ledgers_clean        MISSING on disk, skipping\n",
      " • ledgers_dw           MISSING on disk, skipping\n",
      " • ledgers_enriched     MISSING on disk, skipping\n",
      " • logistics_routes     →  1789 rows ×  4 cols\n",
      " • logistics_routes_clean MISSING on disk, skipping\n",
      " • logistics_routes_dw  MISSING on disk, skipping\n",
      " • logistics_routes_enriched MISSING on disk, skipping\n",
      " • loyalty_programs     → 12345 rows ×  4 cols  (dates: ['start_date'])\n",
      " • loyalty_programs_clean MISSING on disk, skipping\n",
      " • loyalty_programs_dw  MISSING on disk, skipping\n",
      " • loyalty_programs_enriched MISSING on disk, skipping\n",
      " • maintenance_requests →  2890 rows ×  4 cols  (dates: ['request_date'])\n",
      " • maintenance_requests_clean MISSING on disk, skipping\n",
      " • maintenance_requests_dw MISSING on disk, skipping\n",
      " • maintenance_requests_enriched MISSING on disk, skipping\n",
      " • marketing_targets    → 14567 rows ×  5 cols\n",
      " • marketing_targets_clean MISSING on disk, skipping\n",
      " • marketing_targets_dw MISSING on disk, skipping\n",
      " • marketing_targets_enriched MISSING on disk, skipping\n",
      " • opportunities        → 38344 rows ×  4 cols\n",
      " • opportunities_clean  MISSING on disk, skipping\n",
      " • opportunities_dw     MISSING on disk, skipping\n",
      " • opportunities_enriched MISSING on disk, skipping\n",
      " • order_items          → 25341 rows ×  5 cols\n",
      " • order_items_clean    MISSING on disk, skipping\n",
      " • order_items_dw       MISSING on disk, skipping\n",
      " • order_items_enriched MISSING on disk, skipping\n",
      " • orders               → 10000 rows ×  6 cols  (dates: ['order_date'])\n",
      " • orders_backup        → 10000 rows ×  6 cols  (dates: ['order_date'])\n",
      " • orders_clean         MISSING on disk, skipping\n",
      " • orders_dw            MISSING on disk, skipping\n",
      " • orders_enriched      MISSING on disk, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " • org_chart            →  3210 rows ×  3 cols\n",
      " • org_chart_clean      MISSING on disk, skipping\n",
      " • org_chart_dw         MISSING on disk, skipping\n",
      " • org_chart_enriched   MISSING on disk, skipping\n",
      " • packaging            →  1890 rows ×  3 cols\n",
      " • packaging_clean      MISSING on disk, skipping\n",
      " • packaging_dw         MISSING on disk, skipping\n",
      " • packaging_enriched   MISSING on disk, skipping\n",
      " • payments             → 34512 rows ×  5 cols  (dates: ['payment_date'])\n",
      " • payments_clean       MISSING on disk, skipping\n",
      " • payments_dw          MISSING on disk, skipping\n",
      " • payments_enriched    MISSING on disk, skipping\n",
      " • payments_fin         MISSING on disk, skipping\n",
      " • payments_fin_clean   MISSING on disk, skipping\n",
      " • payments_fin_dw      MISSING on disk, skipping\n",
      " • payments_fin_enriched MISSING on disk, skipping\n",
      " • payroll              → 10987 rows ×  5 cols  (dates: ['pay_date'])\n",
      " • payroll_clean        MISSING on disk, skipping\n",
      " • payroll_dw           MISSING on disk, skipping\n",
      " • payroll_enriched     MISSING on disk, skipping\n",
      " • performance_reviews  →  7654 rows ×  4 cols  (dates: ['review_date'])\n",
      " • performance_reviews_clean MISSING on disk, skipping\n",
      " • performance_reviews_dw MISSING on disk, skipping\n",
      " • performance_reviews_enriched MISSING on disk, skipping\n",
      " • predictive_models    → 32109 rows ×  4 cols\n",
      " • predictive_models_clean MISSING on disk, skipping\n",
      " • predictive_models_dw MISSING on disk, skipping\n",
      " • predictive_models_enriched MISSING on disk, skipping\n",
      " • pricing_tiers        →  3456 rows ×  5 cols\n",
      " • pricing_tiers_clean  MISSING on disk, skipping\n",
      " • pricing_tiers_dw     MISSING on disk, skipping\n",
      " • pricing_tiers_enriched MISSING on disk, skipping\n",
      " • product_attributes   →  2890 rows ×  4 cols\n",
      " • product_attributes_clean MISSING on disk, skipping\n",
      " • product_attributes_dw MISSING on disk, skipping\n",
      " • product_attributes_enriched MISSING on disk, skipping\n",
      " • product_images       →  6789 rows ×  3 cols\n",
      " • product_images_clean MISSING on disk, skipping\n",
      " • product_images_dw    MISSING on disk, skipping\n",
      " • product_images_enriched MISSING on disk, skipping\n",
      " • product_reviews      →  9123 rows ×  4 cols\n",
      " • product_reviews_clean MISSING on disk, skipping\n",
      " • product_reviews_dw   MISSING on disk, skipping\n",
      " • product_reviews_enriched MISSING on disk, skipping\n",
      " • products             →  8123 rows ×  6 cols\n",
      " • products_clean       MISSING on disk, skipping\n",
      " • products_dw          MISSING on disk, skipping\n",
      " • products_enriched    MISSING on disk, skipping\n",
      " • profit_centers       → 12345 rows ×  2 cols\n",
      " • profit_centers_clean MISSING on disk, skipping\n",
      " • profit_centers_dw    MISSING on disk, skipping\n",
      " • profit_centers_enriched MISSING on disk, skipping\n",
      " • recruitment_applications →  5432 rows ×  4 cols  (dates: ['application_date'])\n",
      " • recruitment_applications_clean MISSING on disk, skipping\n",
      " • recruitment_applications_dw MISSING on disk, skipping\n",
      " • recruitment_applications_enriched MISSING on disk, skipping\n",
      " • refunds              → 21789 rows ×  5 cols  (dates: ['refund_date'])\n",
      " • refunds_clean        MISSING on disk, skipping\n",
      " • refunds_dw           MISSING on disk, skipping\n",
      " • refunds_enriched     MISSING on disk, skipping\n",
      " • related_products     →  2234 rows ×  3 cols\n",
      " • related_products_clean MISSING on disk, skipping\n",
      " • related_products_dw  MISSING on disk, skipping\n",
      " • related_products_enriched MISSING on disk, skipping\n",
      " • report_configs       →  9876 rows ×  3 cols\n",
      " • report_configs_clean MISSING on disk, skipping\n",
      " • report_configs_dw    MISSING on disk, skipping\n",
      " • report_configs_enriched MISSING on disk, skipping\n",
      " • resolution_times     →  5432 rows ×  3 cols\n",
      " • resolution_times_clean MISSING on disk, skipping\n",
      " • resolution_times_dw  MISSING on disk, skipping\n",
      " • resolution_times_enriched MISSING on disk, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " • sales_channels       → 31245 rows ×  3 cols\n",
      " • sales_channels_clean MISSING on disk, skipping\n",
      " • sales_channels_dw    MISSING on disk, skipping\n",
      " • sales_channels_enriched MISSING on disk, skipping\n",
      " • sales_forecast       → 50123 rows ×  5 cols\n",
      " • sales_forecast_clean MISSING on disk, skipping\n",
      " • sales_forecast_dw    MISSING on disk, skipping\n",
      " • sales_forecast_enriched MISSING on disk, skipping\n",
      " • sales_pipeline       → 63452 rows ×  5 cols  (dates: ['close_date'])\n",
      " • sales_pipeline_clean MISSING on disk, skipping\n",
      " • sales_pipeline_dw    MISSING on disk, skipping\n",
      " • sales_pipeline_enriched MISSING on disk, skipping\n",
      " • sales_regions        → 26789 rows ×  3 cols\n",
      " • sales_regions_clean  MISSING on disk, skipping\n",
      " • sales_regions_dw     MISSING on disk, skipping\n",
      " • sales_regions_enriched MISSING on disk, skipping\n",
      " • sales_targets        → 42301 rows ×  5 cols\n",
      " • sales_targets_clean  MISSING on disk, skipping\n",
      " • sales_targets_dw     MISSING on disk, skipping\n",
      " • sales_targets_enriched MISSING on disk, skipping\n",
      " • satisfaction_surveys → 10987 rows ×  4 cols\n",
      " • satisfaction_surveys_clean MISSING on disk, skipping\n",
      " • satisfaction_surveys_dw MISSING on disk, skipping\n",
      " • satisfaction_surveys_enriched MISSING on disk, skipping\n",
      " • schedule_shifts      →  1987 rows ×  5 cols  (dates: ['shift_date'])\n",
      " • schedule_shifts_clean MISSING on disk, skipping\n",
      " • schedule_shifts_dw   MISSING on disk, skipping\n",
      " • schedule_shifts_enriched MISSING on disk, skipping\n",
      " • segmentation_results → 21098 rows ×  3 cols\n",
      " • segmentation_results_clean MISSING on disk, skipping\n",
      " • segmentation_results_dw MISSING on disk, skipping\n",
      " • segmentation_results_enriched MISSING on disk, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " • seo_keywords         → 10987 rows ×  4 cols\n",
      " • seo_keywords_clean   MISSING on disk, skipping\n",
      " • seo_keywords_dw      MISSING on disk, skipping\n",
      " • seo_keywords_enriched MISSING on disk, skipping\n",
      " • session_data         → 65432 rows ×  4 cols\n",
      " • session_data_clean   MISSING on disk, skipping\n",
      " • session_data_dw      MISSING on disk, skipping\n",
      " • session_data_enriched MISSING on disk, skipping\n",
      " • shipments            →  4876 rows ×  5 cols  (dates: ['shipped_date'])\n",
      " • shipments_clean      MISSING on disk, skipping\n",
      " • shipments_dw         MISSING on disk, skipping\n",
      " • shipments_enriched   MISSING on disk, skipping\n",
      " • social_media_metrics → 17890 rows ×  5 cols\n",
      " • social_media_metrics_clean MISSING on disk, skipping\n",
      " • social_media_metrics_dw MISSING on disk, skipping\n",
      " • social_media_metrics_enriched MISSING on disk, skipping\n",
      " • store_staffing       →  1023 rows ×  5 cols  (dates: ['start_date', 'end_date'])\n",
      " • store_staffing_clean MISSING on disk, skipping\n",
      " • store_staffing_dw    MISSING on disk, skipping\n",
      " • store_staffing_enriched MISSING on disk, skipping\n",
      " • stores               →  4123 rows ×  5 cols\n",
      " • stores_clean         MISSING on disk, skipping\n",
      " • stores_dw            MISSING on disk, skipping\n",
      " • stores_enriched      MISSING on disk, skipping\n",
      " • subcategories        →  4567 rows ×  3 cols\n",
      " • subcategories_clean  MISSING on disk, skipping\n",
      " • subcategories_dw     MISSING on disk, skipping\n",
      " • subcategories_enriched MISSING on disk, skipping\n",
      " • support_agents       →  7654 rows ×  3 cols\n",
      " • support_agents_clean MISSING on disk, skipping\n",
      " • support_agents_dw    MISSING on disk, skipping\n",
      " • support_agents_enriched MISSING on disk, skipping\n",
      " • survey_responses     → 11234 rows ×  5 cols  (dates: ['response_date'])\n",
      " • survey_responses_clean MISSING on disk, skipping\n",
      " • survey_responses_dw  MISSING on disk, skipping\n",
      " • survey_responses_enriched MISSING on disk, skipping\n",
      " • tax_records          → 15000 rows ×  6 cols  (dates: ['tax_date'])\n",
      " • tax_records_clean    MISSING on disk, skipping\n",
      " • tax_records_dw       MISSING on disk, skipping\n",
      " • tax_records_enriched MISSING on disk, skipping\n",
      " • ticket_responses     → 34210 rows ×  4 cols  (dates: ['response_date'])\n",
      " • ticket_responses_clean MISSING on disk, skipping\n",
      " • ticket_responses_dw  MISSING on disk, skipping\n",
      " • ticket_responses_enriched MISSING on disk, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " • tickets              → 45321 rows ×  5 cols\n",
      " • tickets_clean        MISSING on disk, skipping\n",
      " • tickets_dw           MISSING on disk, skipping\n",
      " • tickets_enriched     MISSING on disk, skipping\n",
      " • training_sessions    →  6543 rows ×  4 cols  (dates: ['session_date'])\n",
      " • training_sessions_clean MISSING on disk, skipping\n",
      " • training_sessions_dw MISSING on disk, skipping\n",
      " • training_sessions_enriched MISSING on disk, skipping\n",
      " • transactions_backup  → 30095 rows ×  7 cols\n",
      " • transactions_dw      MISSING on disk, skipping\n",
      " • transfer_orders      →  2678 rows ×  4 cols  (dates: ['transfer_date'])\n",
      " • transfer_orders_clean MISSING on disk, skipping\n",
      " • transfer_orders_dw   MISSING on disk, skipping\n",
      " • transfer_orders_enriched MISSING on disk, skipping\n",
      " • trend_analysis       → 10987 rows ×  3 cols\n",
      " • trend_analysis_clean MISSING on disk, skipping\n",
      " • trend_analysis_dw    MISSING on disk, skipping\n",
      " • trend_analysis_enriched MISSING on disk, skipping\n",
      " • warehouse_locations  →    50 rows ×  7 cols\n",
      " • warehouse_locations_clean MISSING on disk, skipping\n",
      " • warehouse_locations_dw MISSING on disk, skipping\n",
      " • warehouse_locations_enriched MISSING on disk, skipping\n",
      " • web_traffic          → 15643 rows ×  4 cols  (dates: ['visit_date'])\n",
      " • web_traffic_clean    MISSING on disk, skipping\n",
      " • web_traffic_dw       MISSING on disk, skipping\n",
      " • web_traffic_enriched MISSING on disk, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_66052\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# 2) Load CSVs with _only_ real date columns\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "dfs = {}\n",
    "print(\"Loading CSVs…\")\n",
    "for fname in file_list:\n",
    "    path  = DATA_DIR / fname\n",
    "    table = path.stem\n",
    "\n",
    "    if not path.exists():\n",
    "        print(f\" • {table:20s} MISSING on disk, skipping\")\n",
    "        continue\n",
    "\n",
    "    # Peek at columns\n",
    "    sample = pd.read_csv(path, nrows=0)\n",
    "    # Only parse those ending in '_date'\n",
    "    parse_dt = [c for c in sample.columns if c.lower().endswith(\"_date\")]\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        parse_dates=parse_dt,\n",
    "        infer_datetime_format=True\n",
    "    )\n",
    "    dfs[table] = df\n",
    "    print(f\" • {table:20s} → {df.shape[0]:5d} rows × {df.shape[1]:2d} cols\"\n",
    "          + (f\"  (dates: {parse_dt})\" if parse_dt else \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify primary key uniqueness for each table  \n",
    "We infer the primary key for each table by convention (`*_id`) and check that every value is unique, reporting PASS/FAIL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primary Key Checks:\n",
      " • customers            59229/59229 → PASS\n",
      " • products              8123/8123  → PASS\n",
      " • stores                4123/4123  → PASS\n",
      " • distribution_centers  1456/1456  → PASS\n",
      " • categories            2345/2345  → PASS\n",
      " • subcategories         4567/4567  → PASS\n",
      " • product_reviews       9123/9123  → PASS\n",
      " • product_images        6789/6789  → PASS\n",
      " • pricing_tiers         3456/3456  → PASS\n",
      " • inventory_levels      5678/5678  → PASS\n",
      " • product_attributes    2890/2890  → PASS\n",
      " • brand_partners        1345/1345  → PASS\n",
      " • dates                  365/365   → PASS\n",
      " • budgets              MISSING\n",
      " • ledgers              18765/18765 → PASS\n",
      " • orders               10000/10000 → PASS\n",
      " • warehouse_locations     50/50    → PASS\n",
      " • invoices_finance     MISSING\n",
      " • tax_records          15000/15000 → PASS\n",
      " • financial_statements 14234/14234 → PASS\n",
      " • accounts_receivable  16890/16890 → PASS\n",
      " • accounts_payable     15234/15234 → PASS\n",
      " • cost_centers         13456/13456 → PASS\n",
      " • profit_centers       12345/12345 → PASS\n",
      " • currency_rates       16789/16789 → PASS\n",
      " • bank_reconciliations 14532/14532 → PASS\n",
      " • credit_notes         13245/13245 → PASS\n",
      " • payments_legacy      MISSING\n",
      " • campaigns            13245/13245 → PASS\n",
      " • ad_spend             15678/15678 → PASS\n",
      " • social_media_metrics 17890/17890 → PASS\n",
      " • seo_keywords         10987/10987 → PASS\n",
      " • email_campaigns      19876/19876 → PASS\n",
      " • web_traffic          15643/15643 → PASS\n",
      " • loyalty_programs     12345/12345 → PASS\n",
      " • survey_responses     11234/11234 → PASS\n",
      " • marketing_targets    14567/14567 → PASS\n",
      " • customer_segments_marketing 13459/13459 → PASS\n",
      " • employees              200/200   → PASS\n",
      " • attendance_records   18987/18987 → PASS\n",
      " • payroll              10987/10987 → PASS\n",
      " • benefits_enrollment   8456/8456  → PASS\n",
      " • performance_reviews   7654/7654  → PASS\n",
      " • training_sessions     6543/6543  → PASS\n",
      " • recruitment_applications  5432/5432  → PASS\n",
      " • job_postings          4321/4321  → PASS\n",
      " • org_chart             3210/3210  → PASS\n",
      " • compensation_bands    2109/2109  → PASS\n",
      " • dashboard_views      98765/98765 → PASS\n",
      " • kpi_definitions      87654/87654 → PASS\n",
      " • event_logs           76543/76543 → PASS\n",
      " • session_data         65432/65432 → PASS\n",
      " • data_quality_metrics 54321/54321 → PASS\n",
      " • anomaly_detections   43210/43210 → PASS\n",
      " • predictive_models    32109/32109 → PASS\n",
      " • segmentation_results 21098/21098 → PASS\n",
      " • trend_analysis       10987/10987 → PASS\n",
      " • report_configs        9876/9876  → PASS\n",
      " • tickets              45321/45321 → PASS\n",
      " • ticket_responses     34210/34210 → PASS\n",
      " • chat_logs            23109/23109 → PASS\n",
      " • call_records         12098/12098 → PASS\n",
      " • satisfaction_surveys 10987/10987 → PASS\n",
      " • escalations           9876/9876  → PASS\n",
      " • knowledge_base_articles  8765/8765  → PASS\n",
      " • support_agents        7654/7654  → PASS\n",
      " • faq_hits              6543/6543  → PASS\n",
      " • resolution_times      5432/5432  → PASS\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "print(\"\\nPrimary Key Checks:\")\n",
    "for tbl, pk in PKS.items():\n",
    "    df = dfs.get(tbl)\n",
    "    if df is None: \n",
    "        print(f\" • {tbl:20s} MISSING\") \n",
    "        continue\n",
    "    unique = df[pk].nunique()\n",
    "    total  = len(df)\n",
    "    print(f\" • {tbl:20s} {unique:5d}/{total:<5d} →\"\n",
    "          + (\" PASS\" if unique==total else \" FAIL\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check foreign key consistency (with normalized dates)  \n",
    "All `_date` columns are normalized to midnight, then we join each child→parent FK and count any mismatches, reporting PASS or the number of failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Foreign Key Checks (with debug):\n",
      " • Rel# 0: orders         .customer_id     → customers      .customer_id     : PASS\n",
      " • Rel# 1: orders         .store_id        → stores         .store_id        : FAIL (5877)\n",
      " • Rel# 2: order_items    .order_id        → orders         .order_id        : PASS\n",
      " • Rel# 3: order_items    .product_id      → products       .product_id      : PASS\n",
      " • Rel# 4: invoices       .order_id        → orders         .order_id        : PASS\n",
      " • Rel# 5: payments       .invoice_id      → invoices       .invoice_id      : PASS\n",
      " • Rel# 6: refunds        .payment_id      → payments       .payment_id      : PASS\n",
      " • Rel# 7: sales_pipeline .customer_id     → customers      .customer_id     : PASS\n",
      " • Rel# 8: sales_pipeline .opportunity_id  → opportunities  .opportunity_id  : FAIL (25108)\n",
      " • Rel# 9: opportunities  .lead_id         → leads          .lead_id         : PASS\n",
      " • Rel#10: commissions    .opportunity_id  → opportunities  .opportunity_id  : PASS\n",
      " • Rel#11: sales_targets  .sales_forecast_id → sales_forecast .forecast_id     : PASS\n",
      " • Rel#12: inventory_levels.product_id      → products       .product_id      : PASS\n",
      " • Rel#13: inventory_levels.warehouse_id    → warehouse_locations.warehouse_id    : FAIL (5393)\n",
      " • Rel#14: inventory_snapshots.product_id      → products       .product_id      : PASS\n",
      " • Rel#15: inventory_snapshots.warehouse_id    → warehouse_locations.warehouse_id    : FAIL (3068)\n",
      " • Rel#16: shipments      .order_item_id   → order_items    .item_id         : PASS\n",
      " • Rel#17: shipments      .carrier_id      → carriers       .carrier_id      : PASS\n",
      " • Rel#18: maintenance_requests.equipment_id    → equipment_logs .equipment_id    : PASS\n",
      " • Rel#19: store_staffing .store_id        → stores         .store_id        : PASS\n",
      " • Rel#20: store_staffing .employee_id     → employees      .employee_id     : FAIL (798)\n",
      " • Rel#21: schedule_shifts.store_id        → stores         .store_id        : PASS\n",
      " • Rel#22: schedule_shifts.employee_id     → employees      .employee_id     : FAIL (1584)\n",
      " • Rel#23: facility_inspections.store_id        → stores         .store_id        : PASS\n",
      " • Rel#24: equipment_logs .asset_id        → asset_management.asset_id        : PASS\n",
      " • Rel#25: transfer_orders.from_center     → distribution_centers.center_id       : PASS\n",
      " • Rel#26: transfer_orders.to_center       → distribution_centers.center_id       : PASS\n",
      " • Rel#27: packaging      .shipment_id     → shipments      .shipment_id     : PASS\n",
      "⚠️  Rel#28: CHILD 'invoices_finance' missing\n",
      "⚠️  Rel#29: CHILD 'payments_finance' missing, PARENT 'invoices_finance' missing\n",
      " • Rel#30: expense_reports.employee_id     → employees      .employee_id     : FAIL (11655)\n",
      "⚠️  Rel#31: PARENT 'invoices_finance' missing\n",
      " • Rel#32: accounts_receivable.customer_id     → customers      .customer_id     : PASS\n",
      "⚠️  Rel#33: PARENT 'invoices_finance' missing\n",
      " • Rel#34: ad_spend       .campaign_id     → campaigns      .campaign_id     : PASS\n",
      " • Rel#35: email_campaigns.campaign_id     → campaigns      .campaign_id     : PASS\n",
      " • Rel#36: marketing_targets.campaign_id     → campaigns      .campaign_id     : PASS\n",
      " • Rel#37: attendance_records.employee_id     → employees      .employee_id     : FAIL (15192)\n",
      " • Rel#38: payroll        .employee_id     → employees      .employee_id     : FAIL (8738)\n",
      " • Rel#39: benefits_enrollment.employee_id     → employees      .employee_id     : FAIL (6790)\n",
      " • Rel#40: performance_reviews.employee_id     → employees      .employee_id     : FAIL (6159)\n",
      " • Rel#41: org_chart      .manager_id      → employees      .employee_id     : FAIL (2578)\n",
      " • Rel#42: org_chart      .subordinate_id  → employees      .employee_id     : FAIL (2593)\n",
      " • Rel#43: anomaly_detections.metric_id       → data_quality_metrics.metric_id       : PASS\n",
      " • Rel#44: segmentation_results.kpi_id          → kpi_definitions.kpi_id          : PASS\n",
      " • Rel#45: trend_analysis .metric_id       → data_quality_metrics.metric_id       : PASS\n",
      " • Rel#46: tickets        .customer_id     → customers      .customer_id     : PASS\n",
      " • Rel#47: ticket_responses.ticket_id       → tickets        .ticket_id       : PASS\n",
      " • Rel#48: chat_logs      .customer_id     → customers      .customer_id     : PASS\n",
      " • Rel#49: chat_logs      .agent_id        → support_agents .agent_id        : PASS\n",
      " • Rel#50: call_records   .ticket_id       → tickets        .ticket_id       : PASS\n",
      " • Rel#51: satisfaction_surveys.ticket_id       → tickets        .ticket_id       : PASS\n",
      " • Rel#52: escalations    .ticket_id       → tickets        .ticket_id       : PASS\n",
      " • Rel#53: faq_hits       .article_id      → knowledge_base_articles.article_id      : PASS\n",
      " • Rel#54: resolution_times.ticket_id       → tickets        .ticket_id       : PASS\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# 3) Foreign Key Consistency with normalized dates and detailed debugging\n",
    "import numpy as np\n",
    "\n",
    "# 3.a) Normalize any datetime columns ending in '_date'\n",
    "for tbl_name, df in dfs.items():\n",
    "    for col in df.columns:\n",
    "        if col.lower().endswith(\"_date\") and np.issubdtype(df[col].dtype, np.datetime64):\n",
    "            df[col] = df[col].dt.normalize()\n",
    "\n",
    "def fk_issues(child_df, ckey, parent_df, pkey):\n",
    "    return (~child_df[ckey].dropna().isin(parent_df[pkey].dropna())).sum()\n",
    "\n",
    "print(\"\\nForeign Key Checks (with debug):\")\n",
    "for idx, rel in enumerate(relationships):\n",
    "\n",
    "    # ---- debug the raw references field ----\n",
    "    raw_ref = rel.get(\"references\", \"\")\n",
    "    if \".\" not in raw_ref:\n",
    "        print(f\"⚠️  Rel#{idx}: malformed references value (no '.'): {raw_ref!r}  → full rel: {rel}\")\n",
    "        continue\n",
    "\n",
    "    child_table, child_key = rel[\"table\"], rel[\"column\"]\n",
    "    parent_table, parent_key = raw_ref.split(\".\", 1)\n",
    "\n",
    "    df_child = dfs.get(child_table)\n",
    "    df_parent = dfs.get(parent_table)\n",
    "\n",
    "    # Table existence\n",
    "    if df_child is None or df_parent is None:\n",
    "        missing = []\n",
    "        if df_child is None: missing.append(f\"CHILD '{child_table}' missing\")\n",
    "        if df_parent is None: missing.append(f\"PARENT '{parent_table}' missing\")\n",
    "        print(f\"⚠️  Rel#{idx}: {', '.join(missing)}\")\n",
    "        continue\n",
    "\n",
    "    # Column existence\n",
    "    if child_key not in df_child.columns:\n",
    "        print(f\"⚠️  Rel#{idx}: CHILD column missing: {child_table}.{child_key}\")\n",
    "        continue\n",
    "    if parent_key not in df_parent.columns:\n",
    "        print(f\"⚠️  Rel#{idx}: PARENT column missing: {parent_table}.{parent_key}\")\n",
    "        continue\n",
    "\n",
    "    # Compute FK issues\n",
    "    issues = fk_issues(df_child, child_key, df_parent, parent_key)\n",
    "    status = \"PASS\" if issues == 0 else f\"FAIL ({issues})\"\n",
    "    print(f\" • Rel#{idx:2d}: {child_table:15s}.{child_key:15s} → {parent_table:15s}.{parent_key:15s} : {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Foreign Key Checks (flagged):\n",
      " • Rel# 0: orders         .customer_id     → customers      .customer_id     : PASS       [UNINTENTIONAL]\n",
      " • Rel# 1: orders         .store_id        → stores         .store_id        : FAIL(5877) [UNINTENTIONAL]\n",
      " • Rel# 2: order_items    .order_id        → orders         .order_id        : PASS       [UNINTENTIONAL]\n",
      " • Rel# 3: order_items    .product_id      → products       .product_id      : PASS       [UNINTENTIONAL]\n",
      " • Rel# 4: invoices       .order_id        → orders         .order_id        : PASS       [UNINTENTIONAL]\n",
      " • Rel# 5: payments       .invoice_id      → invoices       .invoice_id      : PASS       [UNINTENTIONAL]\n",
      " • Rel# 6: refunds        .payment_id      → payments       .payment_id      : PASS       [UNINTENTIONAL]\n",
      " • Rel# 7: sales_pipeline .customer_id     → customers      .customer_id     : PASS       [UNINTENTIONAL]\n",
      " • Rel# 8: sales_pipeline .opportunity_id  → opportunities  .opportunity_id  : FAIL(25108) [UNINTENTIONAL]\n",
      " • Rel# 9: opportunities  .lead_id         → leads          .lead_id         : PASS       [UNINTENTIONAL]\n",
      " • Rel#10: commissions    .opportunity_id  → opportunities  .opportunity_id  : PASS       [UNINTENTIONAL]\n",
      " • Rel#11: sales_targets  .sales_forecast_id → sales_forecast .forecast_id     : PASS       [UNINTENTIONAL]\n",
      " • Rel#12: inventory_levels.product_id      → products       .product_id      : PASS       [UNINTENTIONAL]\n",
      " • Rel#13: inventory_levels.warehouse_id    → warehouse_locations.warehouse_id    : FAIL(5393) [UNINTENTIONAL]\n",
      " • Rel#14: inventory_snapshots.product_id      → products       .product_id      : PASS       [UNINTENTIONAL]\n",
      " • Rel#15: inventory_snapshots.warehouse_id    → warehouse_locations.warehouse_id    : FAIL(3068) [UNINTENTIONAL]\n",
      " • Rel#16: shipments      .order_item_id   → order_items    .item_id         : PASS       [UNINTENTIONAL]\n",
      " • Rel#17: shipments      .carrier_id      → carriers       .carrier_id      : PASS       [UNINTENTIONAL]\n",
      " • Rel#18: maintenance_requests.equipment_id    → equipment_logs .equipment_id    : PASS       [UNINTENTIONAL]\n",
      " • Rel#19: store_staffing .store_id        → stores         .store_id        : PASS       [UNINTENTIONAL]\n",
      " • Rel#20: store_staffing .employee_id     → employees      .employee_id     : FAIL(798)  [UNINTENTIONAL]\n",
      " • Rel#21: schedule_shifts.store_id        → stores         .store_id        : PASS       [UNINTENTIONAL]\n",
      " • Rel#22: schedule_shifts.employee_id     → employees      .employee_id     : FAIL(1584) [UNINTENTIONAL]\n",
      " • Rel#23: facility_inspections.store_id        → stores         .store_id        : PASS       [UNINTENTIONAL]\n",
      " • Rel#24: equipment_logs .asset_id        → asset_management.asset_id        : PASS       [UNINTENTIONAL]\n",
      " • Rel#25: transfer_orders.from_center     → distribution_centers.center_id       : PASS       [UNINTENTIONAL]\n",
      " • Rel#26: transfer_orders.to_center       → distribution_centers.center_id       : PASS       [UNINTENTIONAL]\n",
      " • Rel#27: packaging      .shipment_id     → shipments      .shipment_id     : PASS       [UNINTENTIONAL]\n",
      "⚠️ Rel#28: Missing table → child:invoices_finance, parent:billing_cycles\n",
      "⚠️ Rel#29: Missing table → child:payments_finance, parent:invoices_finance\n",
      " • Rel#30: expense_reports.employee_id     → employees      .employee_id     : FAIL(11655) [UNINTENTIONAL]\n",
      "⚠️ Rel#31: Missing table → child:tax_records, parent:invoices_finance\n",
      " • Rel#32: accounts_receivable.customer_id     → customers      .customer_id     : PASS       [UNINTENTIONAL]\n",
      "⚠️ Rel#33: Missing table → child:credit_notes, parent:invoices_finance\n",
      " • Rel#34: ad_spend       .campaign_id     → campaigns      .campaign_id     : PASS       [UNINTENTIONAL]\n",
      " • Rel#35: email_campaigns.campaign_id     → campaigns      .campaign_id     : PASS       [UNINTENTIONAL]\n",
      " • Rel#36: marketing_targets.campaign_id     → campaigns      .campaign_id     : PASS       [UNINTENTIONAL]\n",
      " • Rel#37: attendance_records.employee_id     → employees      .employee_id     : FAIL(15192) [UNINTENTIONAL]\n",
      " • Rel#38: payroll        .employee_id     → employees      .employee_id     : FAIL(8738) [UNINTENTIONAL]\n",
      " • Rel#39: benefits_enrollment.employee_id     → employees      .employee_id     : FAIL(6790) [UNINTENTIONAL]\n",
      " • Rel#40: performance_reviews.employee_id     → employees      .employee_id     : FAIL(6159) [UNINTENTIONAL]\n",
      " • Rel#41: org_chart      .manager_id      → employees      .employee_id     : FAIL(2578) [UNINTENTIONAL]\n",
      " • Rel#42: org_chart      .subordinate_id  → employees      .employee_id     : FAIL(2593) [UNINTENTIONAL]\n",
      " • Rel#43: anomaly_detections.metric_id       → data_quality_metrics.metric_id       : PASS       [UNINTENTIONAL]\n",
      " • Rel#44: segmentation_results.kpi_id          → kpi_definitions.kpi_id          : PASS       [UNINTENTIONAL]\n",
      " • Rel#45: trend_analysis .metric_id       → data_quality_metrics.metric_id       : PASS       [UNINTENTIONAL]\n",
      " • Rel#46: tickets        .customer_id     → customers      .customer_id     : PASS       [UNINTENTIONAL]\n",
      " • Rel#47: ticket_responses.ticket_id       → tickets        .ticket_id       : PASS       [UNINTENTIONAL]\n",
      " • Rel#48: chat_logs      .customer_id     → customers      .customer_id     : PASS       [UNINTENTIONAL]\n",
      " • Rel#49: chat_logs      .agent_id        → support_agents .agent_id        : PASS       [UNINTENTIONAL]\n",
      " • Rel#50: call_records   .ticket_id       → tickets        .ticket_id       : PASS       [UNINTENTIONAL]\n",
      " • Rel#51: satisfaction_surveys.ticket_id       → tickets        .ticket_id       : PASS       [UNINTENTIONAL]\n",
      " • Rel#52: escalations    .ticket_id       → tickets        .ticket_id       : PASS       [UNINTENTIONAL]\n",
      " • Rel#53: faq_hits       .article_id      → knowledge_base_articles.article_id      : PASS       [UNINTENTIONAL]\n",
      " • Rel#54: resolution_times.ticket_id       → tickets        .ticket_id       : PASS       [UNINTENTIONAL]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "\n",
    "# ——————————————————————————————\n",
    "# 0) Build set of intentionally broken-FK pairs\n",
    "# ——————————————————————————————\n",
    "anomaly_rels = set()\n",
    "\n",
    "# (a) from tables.yml: if you have an \"anomalies\" section per table\n",
    "for tname, tdef in tbl_cfg[\"tables\"].items():\n",
    "    table_anoms = tdef.get(\"anomalies\", {})\n",
    "    # e.g. under table_anoms[\"foreign_key\"] you list columns to break\n",
    "    for col in table_anoms.get(\"foreign_key\", []):\n",
    "        anomaly_rels.add((tname, col))\n",
    "    # or if you configured anomalies per-column:\n",
    "    for col, cdef in tdef[\"columns\"].items():\n",
    "        rate = cdef.get(\"anomaly_rate\", 0)\n",
    "        if rate and cdef.get(\"type\") == \"int\":  # or whatever your marker is\n",
    "            anomaly_rels.add((tname, col))\n",
    "\n",
    "# (b) from lineage.yml: if you injected anomalies at the step level\n",
    "for step in lin_cfg[\"steps\"]:\n",
    "    for a in step.get(\"anomalies\", []):\n",
    "        if a.get(\"type\") == \"fk\":\n",
    "            anomaly_rels.add((a[\"table\"], a[\"column\"]))\n",
    "\n",
    "\n",
    "# ——————————————————————————————\n",
    "# 1) Normalize dates\n",
    "# ——————————————————————————————\n",
    "for tbl_name, df in dfs.items():\n",
    "    for col in df.columns:\n",
    "        if col.lower().endswith(\"_date\") and np.issubdtype(df[col].dtype, np.datetime64):\n",
    "            df[col] = df[col].dt.normalize()\n",
    "\n",
    "\n",
    "# ——————————————————————————————\n",
    "# 2) FK‐check with anomaly flags\n",
    "# ——————————————————————————————\n",
    "def fk_issues(child_df, ckey, parent_df, pkey):\n",
    "    return (~child_df[ckey].dropna().isin(parent_df[pkey].dropna())).sum()\n",
    "\n",
    "print(\"\\nForeign Key Checks (flagged):\")\n",
    "for idx, rel in enumerate(relationships):\n",
    "    raw_ref = rel.get(\"references\", \"\")\n",
    "    if \".\" not in raw_ref:\n",
    "        print(f\"⚠️ Rel#{idx}: malformed references (no '.'): {raw_ref!r}\")\n",
    "        continue\n",
    "\n",
    "    child_table, child_key = rel[\"table\"], rel[\"column\"]\n",
    "    parent_table, parent_key = raw_ref.split(\".\", 1)\n",
    "\n",
    "    df_child = dfs.get(child_table)\n",
    "    df_parent = dfs.get(parent_table)\n",
    "    if df_child is None or df_parent is None:\n",
    "        print(f\"⚠️ Rel#{idx}: Missing table → child:{child_table}, parent:{parent_table}\")\n",
    "        continue\n",
    "\n",
    "    if child_key not in df_child.columns:\n",
    "        print(f\"⚠️ Rel#{idx}: Missing child column {child_table}.{child_key}\")\n",
    "        continue\n",
    "    if parent_key not in df_parent.columns:\n",
    "        print(f\"⚠️ Rel#{idx}: Missing parent column {parent_table}.{parent_key}\")\n",
    "        continue\n",
    "\n",
    "    issues = fk_issues(df_child, child_key, df_parent, parent_key)\n",
    "    status = \"PASS\" if issues == 0 else f\"FAIL({issues})\"\n",
    "\n",
    "    intentional = (child_table, child_key) in anomaly_rels\n",
    "    flag = \"INTENTIONAL\" if intentional else \"UNINTENTIONAL\"\n",
    "\n",
    "    print(f\" • Rel#{idx:2d}: \"\n",
    "          f\"{child_table:15s}.{child_key:15s} → \"\n",
    "          f\"{parent_table:15s}.{parent_key:15s} : \"\n",
    "          f\"{status:10s} [{flag}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage CSVs and load tables into Snowflake  \n",
    "We PUT each CSV into the user stage, then use Snowpark (with `parse_header` option) and our `schema_metadata.json` to CREATE/OVERWRITE each table in Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_and_fix_csv_issues(data_dir, schema_meta, verbose=True):\n",
    "    \"\"\"\n",
    "    Check for and fix common CSV issues that could cause Snowflake loading problems.\n",
    "    Focuses on quoting issues, data type mismatches, and other common problems.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to directory containing CSV files\n",
    "        schema_meta: Dictionary containing schema metadata\n",
    "        verbose: Whether to print detailed diagnostic information\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with statistics of issues found and fixed\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import re\n",
    "    \n",
    "    stats = {\n",
    "        \"files_checked\": 0,\n",
    "        \"files_fixed\": 0,\n",
    "        \"total_issues\": 0,\n",
    "        \"issues_by_type\": {},\n",
    "        \"files_with_issues\": []\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n==== CSV File Diagnostic Tool ====\")\n",
    "    \n",
    "    # Loop through each table in the schema\n",
    "    for table_name, table_info in schema_meta[\"tables\"].items():\n",
    "        csv_path = Path(data_dir) / f\"{table_name}.csv\"\n",
    "        \n",
    "        # Skip if file doesn't exist\n",
    "        if not csv_path.exists():\n",
    "            if verbose:\n",
    "                print(f\"❌ File not found: {csv_path}\")\n",
    "            continue\n",
    "        \n",
    "        stats[\"files_checked\"] += 1\n",
    "        file_issues = []\n",
    "        \n",
    "        # Get expected column types from schema\n",
    "        expected_types = {}\n",
    "        for col_name, col_meta in table_info[\"columns\"].items():\n",
    "            raw_type = col_meta if isinstance(col_meta, str) else col_meta.get(\"type\", \"string\")\n",
    "            # Normalize type names\n",
    "            if \"int\" in raw_type.lower():\n",
    "                expected_types[col_name] = \"int\"\n",
    "            elif any(t in raw_type.lower() for t in [\"float\", \"decimal\", \"double\", \"numeric\"]):\n",
    "                expected_types[col_name] = \"float\"\n",
    "            elif \"date\" in raw_type.lower():\n",
    "                expected_types[col_name] = \"date\"\n",
    "            else:\n",
    "                expected_types[col_name] = \"string\"\n",
    "        \n",
    "        # Read the CSV file\n",
    "        try:\n",
    "            # First try with standard reading\n",
    "            df = pd.read_csv(csv_path)\n",
    "            \n",
    "            # Check for file-level issues\n",
    "            if len(df.columns) == 1 and df.columns[0].count(',') > 0:\n",
    "                # Likely delimiter issue\n",
    "                if verbose:\n",
    "                    print(f\"🔍 {table_name}.csv: Possible delimiter issue detected\")\n",
    "                # Try again with different parser\n",
    "                df = pd.read_csv(csv_path, engine='python')\n",
    "                file_issues.append(\"delimiter_issue\")\n",
    "            \n",
    "            # Check for columns that should be numeric but have string values\n",
    "            for col_name, expected_type in expected_types.items():\n",
    "                if col_name not in df.columns:\n",
    "                    continue\n",
    "                    \n",
    "                if expected_type in [\"int\", \"float\"]:\n",
    "                    # Try converting to numeric to check for issues\n",
    "                    try:\n",
    "                        pd.to_numeric(df[col_name])\n",
    "                    except Exception:\n",
    "                        # Found non-numeric values\n",
    "                        if verbose:\n",
    "                            print(f\"⚠️ {table_name}.csv: Column '{col_name}' should be {expected_type} but has non-numeric values\")\n",
    "                        \n",
    "                        # Get problematic rows\n",
    "                        problem_mask = pd.to_numeric(df[col_name], errors='coerce').isna() & ~df[col_name].isna()\n",
    "                        problem_rows = df[problem_mask]\n",
    "                        \n",
    "                        if verbose and not problem_rows.empty:\n",
    "                            print(f\"   Found {len(problem_rows)} problem rows. Sample values:\")\n",
    "                            for idx, row in problem_rows.head(3).iterrows():\n",
    "                                print(f\"   - Row {idx}: '{row[col_name]}'\")\n",
    "                        \n",
    "                        # Check for quoted values with commas\n",
    "                        quote_comma_pattern = r'\"[^\"]*,[^\"]*\"'\n",
    "                        if problem_rows.empty:\n",
    "                            continue\n",
    "                            \n",
    "                        if any(problem_rows[col_name].astype(str).str.contains(quote_comma_pattern)):\n",
    "                            if verbose:\n",
    "                                print(f\"   Issue likely caused by quoted values with commas\")\n",
    "                            file_issues.append(f\"quoted_comma_in_{col_name}\")\n",
    "                            \n",
    "                            # Fix: Extract just the numeric part if a pattern is detected\n",
    "                            # This is a simplified fix - assumes numeric values are at start of string\n",
    "                            numeric_extraction = df[col_name].astype(str).str.extract(r'(\\d+)')\n",
    "                            mask = ~numeric_extraction[0].isna()\n",
    "                            if mask.sum() > 0:\n",
    "                                df.loc[mask, col_name] = numeric_extraction[0]\n",
    "                        \n",
    "                        # Fix: Convert any remaining problematic values to NaN\n",
    "                        df[col_name] = pd.to_numeric(df[col_name], errors='coerce')\n",
    "                        file_issues.append(f\"type_mismatch_in_{col_name}\")\n",
    "            \n",
    "            # Check for quoting issues (unbalanced quotes)\n",
    "            for col_name in df.columns:\n",
    "                if df[col_name].dtype == 'object':\n",
    "                    # Count quotes in each cell\n",
    "                    quote_counts = df[col_name].astype(str).str.count('\"')\n",
    "                    if (quote_counts % 2 != 0).any():\n",
    "                        if verbose:\n",
    "                            print(f\"⚠️ {table_name}.csv: Column '{col_name}' has unbalanced quotes\")\n",
    "                        file_issues.append(f\"unbalanced_quotes_in_{col_name}\")\n",
    "                        \n",
    "                        # Fix: Balance quotes by adding missing quote or removing extra\n",
    "                        for idx, count in quote_counts[quote_counts % 2 != 0].items():\n",
    "                            val = str(df.at[idx, col_name])\n",
    "                            if count % 2 == 1:  # Odd number of quotes\n",
    "                                df.at[idx, col_name] = val + '\"' if val.count('\"') % 2 == 1 else val[:-1]\n",
    "            \n",
    "            # If any issues were found, write the fixed file\n",
    "            if file_issues:\n",
    "                fixed_path = csv_path.with_suffix('.fixed.csv')\n",
    "                df.to_csv(fixed_path, index=False, quoting=1)  # quoting=1 means quote all non-numeric\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"✅ Fixed file written to: {fixed_path}\")\n",
    "                    print(f\"   Issues fixed: {', '.join(file_issues)}\")\n",
    "                \n",
    "                # Replace original with fixed version\n",
    "                import shutil\n",
    "                shutil.move(fixed_path, csv_path)\n",
    "                \n",
    "                stats[\"files_fixed\"] += 1\n",
    "                stats[\"total_issues\"] += len(file_issues)\n",
    "                stats[\"files_with_issues\"].append(table_name)\n",
    "                \n",
    "                # Count by issue type\n",
    "                for issue in file_issues:\n",
    "                    stats[\"issues_by_type\"][issue] = stats[\"issues_by_type\"].get(issue, 0) + 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"❌ Error processing {table_name}.csv: {str(e)}\")\n",
    "            stats[\"issues_by_type\"][\"process_error\"] = stats[\"issues_by_type\"].get(\"process_error\", 0) + 1\n",
    "    \n",
    "    # Print summary\n",
    "    if verbose:\n",
    "        print(\"\\n==== CSV Diagnostic Summary ====\")\n",
    "        print(f\"Files checked: {stats['files_checked']}\")\n",
    "        print(f\"Files with issues: {stats['files_fixed']}\")\n",
    "        print(f\"Total issues fixed: {stats['total_issues']}\")\n",
    "        \n",
    "        if stats[\"issues_by_type\"]:\n",
    "            print(\"\\nIssues by type:\")\n",
    "            for issue_type, count in stats[\"issues_by_type\"].items():\n",
    "                print(f\"  - {issue_type}: {count}\")\n",
    "        \n",
    "        if stats[\"files_with_issues\"]:\n",
    "            print(\"\\nTables fixed:\")\n",
    "            for table in stats[\"files_with_issues\"]:\n",
    "                print(f\"  - {table}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def prepare_snowflake_loading(sess, schema_meta, data_dir):\n",
    "    \"\"\"\n",
    "    Prepare for Snowflake loading by dropping existing tables and \n",
    "    creating a properly configured reader that handles CSV issues.\n",
    "    \n",
    "    Args:\n",
    "        sess: Snowflake session\n",
    "        schema_meta: Dictionary with schema metadata\n",
    "        data_dir: Directory containing CSV files\n",
    "    \n",
    "    Returns:\n",
    "        Configured reader for loading CSVs\n",
    "    \"\"\"\n",
    "    # First drop existing tables to avoid conflicts\n",
    "    print(\"\\nDropping existing tables...\")\n",
    "    for tbl_name in schema_meta[\"tables\"].keys():\n",
    "        table_ident = f'\"{tbl_name.upper()}\"'\n",
    "        try:\n",
    "            sess.sql(f\"DROP TABLE IF EXISTS {table_ident}\").collect()\n",
    "            print(f\"  • Dropped {table_ident}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  • Failed to drop {table_ident}: {str(e)}\")\n",
    "    \n",
    "    # Create a robust reader with proper options\n",
    "    reader = sess.read \\\n",
    "        .option(\"parse_header\", True) \\\n",
    "        .option(\"field_delimiter\", \",\") \\\n",
    "        .option(\"quote\", \"\\\"\") \\\n",
    "        .option(\"escape\", \"\\\"\") \\\n",
    "        .option(\"encoding\", \"UTF-8\") \\\n",
    "        .option(\"skip_blank_lines\", True) \\\n",
    "        .option(\"empty_field_as_null\", True)\n",
    "    \n",
    "    print(\"\\nConfigured robust CSV reader with proper quoting and escaping\")\n",
    "    \n",
    "    return reader\n",
    "\n",
    "# Usage example:\n",
    "# 1. First diagnose and fix CSV files\n",
    "# diagnose_and_fix_csv_issues(DATA_DIR, schema_meta)\n",
    "#\n",
    "# 2. Then prepare for loading and get a robust reader\n",
    "# reader = prepare_snowflake_loading(sess, schema_meta, DATA_DIR)\n",
    "#\n",
    "# 3. Use the reader in your loading code:\n",
    "# for tbl_name, tbl_info in schema_meta[\"tables\"].items():\n",
    "#     ...\n",
    "#     df_snow = reader.schema(schema).csv(f\"@~/{tbl_name}.csv\")\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Diagnosing and fixing CSV issues...\n",
      "\n",
      "==== CSV File Diagnostic Tool ====\n",
      "\n",
      "==== CSV Diagnostic Summary ====\n",
      "Files checked: 69\n",
      "Files with issues: 0\n",
      "Total issues fixed: 0\n",
      "\n",
      "2. Staging verified CSVs to Snowflake...\n",
      " • PUT accounts_payable.csv\n",
      " • PUT accounts_receivable.csv\n",
      " • PUT ad_spend.csv\n",
      " • PUT anomaly_detections.csv\n",
      " • PUT asset_management.csv\n",
      " • PUT attendance_records.csv\n",
      " • PUT bank_reconciliations.csv\n",
      " • PUT benefits_enrollment.csv\n",
      " • PUT billing_cycles.csv\n",
      " • PUT brand_partners.csv\n",
      " • PUT budgets.csv\n",
      " • PUT call_records.csv\n",
      " • PUT campaigns.csv\n",
      " • PUT carriers.csv\n",
      " • PUT categories.csv\n",
      " • PUT chat_logs.csv\n",
      " • PUT commissions.csv\n",
      " • PUT compensation_bands.csv\n",
      " • PUT cost_centers.csv\n",
      " • PUT credit_notes.csv\n",
      " • PUT currency_rates.csv\n",
      " • PUT customers.csv\n",
      " • PUT customer_segments.csv\n",
      " • PUT customer_segments_marketing.csv\n",
      " • PUT dashboard_views.csv\n",
      " • PUT data_quality_metrics.csv\n",
      " • PUT dates.csv\n",
      " • PUT distribution_centers.csv\n",
      " • PUT email_campaigns.csv\n",
      " • PUT employees.csv\n",
      " • PUT equipment_logs.csv\n",
      " • PUT escalations.csv\n",
      " • PUT event_logs.csv\n",
      " • PUT expense_reports.csv\n",
      " • PUT facility_inspections.csv\n",
      " • PUT faq_hits.csv\n",
      " • PUT financial_statements.csv\n",
      " • PUT inventory.csv\n",
      " • PUT inventory_levels.csv\n",
      " • PUT inventory_snapshots.csv\n",
      " • PUT invoices.csv\n",
      " • PUT invoices_finance.csv\n",
      " • PUT job_postings.csv\n",
      " • PUT knowledge_base_articles.csv\n",
      " • PUT kpi_definitions.csv\n",
      " • PUT leads.csv\n",
      " • PUT ledgers.csv\n",
      " • PUT legacy_customers.csv\n",
      " • PUT logistics_routes.csv\n",
      " • PUT loyalty_programs.csv\n",
      " • PUT maintenance_requests.csv\n",
      " • PUT marketing_targets.csv\n",
      " • PUT opportunities.csv\n",
      " • PUT orders.csv\n",
      " • PUT orders_backup.csv\n",
      " • PUT order_items.csv\n",
      " • PUT org_chart.csv\n",
      " • PUT packaging.csv\n",
      " • PUT payments.csv\n",
      " • PUT payments_finance.csv\n",
      " • PUT payments_legacy.csv\n",
      " • PUT payroll.csv\n",
      " • PUT performance_reviews.csv\n",
      " • PUT predictive_models.csv\n",
      " • PUT pricing_tiers.csv\n",
      " • PUT products.csv\n",
      " • PUT product_attributes.csv\n",
      " • PUT product_images.csv\n",
      " • PUT product_reviews.csv\n",
      " • PUT profit_centers.csv\n",
      " • PUT promotions.csv\n",
      " • PUT quotes.csv\n",
      " • PUT recruitment_applications.csv\n",
      " • PUT refunds.csv\n",
      " • PUT related_products.csv\n",
      " • PUT report_configs.csv\n",
      " • PUT resolution_times.csv\n",
      " • PUT returns.csv\n",
      " • PUT sales_channels.csv\n",
      " • PUT sales_forecast.csv\n",
      " • PUT sales_pipeline.csv\n",
      " • PUT sales_regions.csv\n",
      " • PUT sales_targets.csv\n",
      " • PUT satisfaction_surveys.csv\n",
      " • PUT schedule_shifts.csv\n",
      " • PUT segmentation_results.csv\n",
      " • PUT seo_keywords.csv\n",
      " • PUT session_data.csv\n",
      " • PUT shipments.csv\n",
      " • PUT social_media_metrics.csv\n",
      " • PUT stores.csv\n",
      " • PUT store_staffing.csv\n",
      " • PUT subcategories.csv\n",
      " • PUT suppliers.csv\n",
      " • PUT support_agents.csv\n",
      " • PUT survey_responses.csv\n",
      " • PUT tax_records.csv\n",
      " • PUT tickets.csv\n",
      " • PUT ticket_responses.csv\n",
      " • PUT training_sessions.csv\n",
      " • PUT transactions.csv\n",
      " • PUT transactions_backup.csv\n",
      " • PUT transfer_orders.csv\n",
      " • PUT trend_analysis.csv\n",
      " • PUT warehouse_locations.csv\n",
      " • PUT web_traffic.csv\n",
      "\n",
      "3. Preparing Snowflake environment...\n",
      "\n",
      "3a. Creating Snowflake reader with compatible quote settings...\n",
      "\n",
      "3b. Dropping existing tables...\n",
      "  • Dropped \"RESOLUTION_TIMES\"\n",
      "  • Dropped \"FAQ_HITS\"\n",
      "  • Dropped \"KNOWLEDGE_BASE_ARTICLES\"\n",
      "  • Dropped \"ESCALATIONS\"\n",
      "  • Dropped \"SATISFACTION_SURVEYS\"\n",
      "  • Dropped \"CALL_RECORDS\"\n",
      "  • Dropped \"CHAT_LOGS\"\n",
      "  • Dropped \"SUPPORT_AGENTS\"\n",
      "  • Dropped \"TICKET_RESPONSES\"\n",
      "  • Dropped \"TICKETS\"\n",
      "  • Dropped \"REPORT_CONFIGS\"\n",
      "  • Dropped \"TREND_ANALYSIS\"\n",
      "  • Dropped \"SEGMENTATION_RESULTS\"\n",
      "  • Dropped \"PREDICTIVE_MODELS\"\n",
      "  • Dropped \"ANOMALY_DETECTIONS\"\n",
      "  • Dropped \"DATA_QUALITY_METRICS\"\n",
      "  • Dropped \"SESSION_DATA\"\n",
      "  • Dropped \"EVENT_LOGS\"\n",
      "  • Dropped \"KPI_DEFINITIONS\"\n",
      "  • Dropped \"DASHBOARD_VIEWS\"\n",
      "  • Dropped \"COMPENSATION_BANDS\"\n",
      "  • Dropped \"ORG_CHART\"\n",
      "  • Dropped \"JOB_POSTINGS\"\n",
      "  • Dropped \"RECRUITMENT_APPLICATIONS\"\n",
      "  • Dropped \"TRAINING_SESSIONS\"\n",
      "  • Dropped \"PERFORMANCE_REVIEWS\"\n",
      "  • Dropped \"BENEFITS_ENROLLMENT\"\n",
      "  • Dropped \"PAYROLL\"\n",
      "  • Dropped \"ATTENDANCE_RECORDS\"\n",
      "  • Dropped \"CUSTOMER_SEGMENTS_MARKETING\"\n",
      "  • Dropped \"MARKETING_TARGETS\"\n",
      "  • Dropped \"SURVEY_RESPONSES\"\n",
      "  • Dropped \"LOYALTY_PROGRAMS\"\n",
      "  • Dropped \"WEB_TRAFFIC\"\n",
      "  • Dropped \"EMAIL_CAMPAIGNS\"\n",
      "  • Dropped \"SEO_KEYWORDS\"\n",
      "  • Dropped \"SOCIAL_MEDIA_METRICS\"\n",
      "  • Dropped \"AD_SPEND\"\n",
      "  • Dropped \"CAMPAIGNS\"\n",
      "  • Dropped \"PAYMENTS_LEGACY\"\n",
      "  • Dropped \"CREDIT_NOTES\"\n",
      "  • Dropped \"BANK_RECONCILIATIONS\"\n",
      "  • Dropped \"CURRENCY_RATES\"\n",
      "  • Dropped \"PROFIT_CENTERS\"\n",
      "  • Dropped \"COST_CENTERS\"\n",
      "  • Dropped \"ACCOUNTS_PAYABLE\"\n",
      "  • Dropped \"ACCOUNTS_RECEIVABLE\"\n",
      "  • Dropped \"FINANCIAL_STATEMENTS\"\n",
      "  • Dropped \"TAX_RECORDS\"\n",
      "  • Dropped \"INVOICES_FINANCE\"\n",
      "  • Dropped \"ORDERS\"\n",
      "  • Dropped \"LEDGERS\"\n",
      "  • Dropped \"BUDGETS\"\n",
      "  • Dropped \"DATES\"\n",
      "  • Dropped \"RELATED_PRODUCTS\"\n",
      "  • Dropped \"BRAND_PARTNERS\"\n",
      "  • Dropped \"PRODUCT_ATTRIBUTES\"\n",
      "  • Dropped \"INVENTORY_LEVELS\"\n",
      "  • Dropped \"WAREHOUSE_LOCATIONS\"\n",
      "  • Dropped \"PRICING_TIERS\"\n",
      "  • Dropped \"PRODUCT_IMAGES\"\n",
      "  • Dropped \"PRODUCT_REVIEWS\"\n",
      "  • Dropped \"SUBCATEGORIES\"\n",
      "  • Dropped \"CATEGORIES\"\n",
      "  • Dropped \"DISTRIBUTION_CENTERS\"\n",
      "  • Dropped \"EMPLOYEES\"\n",
      "  • Dropped \"STORES\"\n",
      "  • Dropped \"PRODUCTS\"\n",
      "  • Dropped \"CUSTOMERS\"\n",
      "\n",
      "4. Loading tables into Snowflake...\n",
      " • \"RESOLUTION_TIMES\"             ✓\n",
      " • \"FAQ_HITS\"                     ✓\n",
      " • \"KNOWLEDGE_BASE_ARTICLES\"      ✓\n",
      " • \"ESCALATIONS\"                  ✓\n",
      " • \"SATISFACTION_SURVEYS\"         ✓\n",
      " • \"CALL_RECORDS\"                 ✓\n",
      " • \"CHAT_LOGS\"                    ✓\n",
      " • \"SUPPORT_AGENTS\"               ✓\n",
      " • \"TICKET_RESPONSES\"             ✓\n",
      " • \"TICKETS\"                      ✓\n",
      " • \"REPORT_CONFIGS\"               ✓\n",
      " • \"TREND_ANALYSIS\"               ✓\n",
      " • \"SEGMENTATION_RESULTS\"         ✓\n",
      " • \"PREDICTIVE_MODELS\"            ✓\n",
      " • \"ANOMALY_DETECTIONS\"           ✓\n",
      " • \"DATA_QUALITY_METRICS\"         ✓\n",
      " • \"SESSION_DATA\"                 ✓\n",
      " • \"EVENT_LOGS\"                   ✓\n",
      " • \"KPI_DEFINITIONS\"              ✓\n",
      " • \"DASHBOARD_VIEWS\"              ✓\n",
      " • \"COMPENSATION_BANDS\"           ✓\n",
      " • \"ORG_CHART\"                    ✓\n",
      " • \"JOB_POSTINGS\"                 ✓\n",
      " • \"RECRUITMENT_APPLICATIONS\"     ✓\n",
      " • \"TRAINING_SESSIONS\"            ✓\n",
      " • \"PERFORMANCE_REVIEWS\"          ✓\n",
      " • \"BENEFITS_ENROLLMENT\"          ✓\n",
      " • \"PAYROLL\"                      ✓\n",
      " • \"ATTENDANCE_RECORDS\"           ✓\n",
      " • \"CUSTOMER_SEGMENTS_MARKETING\"  ✓\n",
      " • \"MARKETING_TARGETS\"            ✓\n",
      " • \"SURVEY_RESPONSES\"             ✓\n",
      " • \"LOYALTY_PROGRAMS\"             ✓\n",
      " • \"WEB_TRAFFIC\"                  ✓\n",
      " • \"EMAIL_CAMPAIGNS\"              ✓\n",
      " • \"SEO_KEYWORDS\"                 ✓\n",
      " • \"SOCIAL_MEDIA_METRICS\"         ✓\n",
      " • \"AD_SPEND\"                     ✓\n",
      " • \"CAMPAIGNS\"                    ✓\n",
      " • \"PAYMENTS_LEGACY\"              ✓\n",
      " • \"CREDIT_NOTES\"                 ✓\n",
      " • \"BANK_RECONCILIATIONS\"         ✓\n",
      " • \"CURRENCY_RATES\"               ✓\n",
      " • \"PROFIT_CENTERS\"               ✓\n",
      " • \"COST_CENTERS\"                 ✓\n",
      " • \"ACCOUNTS_PAYABLE\"             ✓\n",
      " • \"ACCOUNTS_RECEIVABLE\"          ✓\n",
      " • \"FINANCIAL_STATEMENTS\"         ✓\n",
      " • \"TAX_RECORDS\"                  ✓\n",
      " • \"INVOICES_FINANCE\"             ✓\n",
      " • \"ORDERS\"                       ✓\n",
      " • \"LEDGERS\"                      ✓\n",
      " • \"BUDGETS\"                      ✓\n",
      " • \"DATES\"                        ✓\n",
      " • \"RELATED_PRODUCTS\"             ✓\n",
      " • \"BRAND_PARTNERS\"               ✓\n",
      " • \"PRODUCT_ATTRIBUTES\"           ✓\n",
      " • \"INVENTORY_LEVELS\"             ✓\n",
      " • \"WAREHOUSE_LOCATIONS\"          ✓\n",
      " • \"PRICING_TIERS\"                ✓\n",
      " • \"PRODUCT_IMAGES\"               ✓\n",
      " • \"PRODUCT_REVIEWS\"              ✓\n",
      " • \"SUBCATEGORIES\"                ✓\n",
      " • \"CATEGORIES\"                   ✓\n",
      " • \"DISTRIBUTION_CENTERS\"         ✓\n",
      " • \"EMPLOYEES\"                    ✓\n",
      " • \"STORES\"                       ✓\n",
      " • \"PRODUCTS\"                     ✓\n",
      " • \"CUSTOMERS\"                    ✓\n",
      "\n",
      "===== Snowflake Load Summary =====\n",
      "Total tables processed: 69\n",
      "Successfully loaded: 69\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 5) Staging & Loading to Snowflake with CSV diagnostics\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, FloatType,\n",
    "    StringType, DateType,\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Snowflake connection\n",
    "SF_CONN = {\n",
    "    \"account\":        os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"user\":           os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    \"password\":       os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"role\":           os.getenv(\"SNOWFLAKE_ROLE\"),\n",
    "    \"warehouse\":      os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"database\":       os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"schema\":         os.getenv(\"SNOWFLAKE_SCHEMA\"),\n",
    "    \"ocsp_fail_open\": os.getenv(\"SNOWFLAKE_OCSP_FAIL_OPEN\", \"True\").lower() == \"true\",\n",
    "    \"insecure_mode\":  os.getenv(\"SNOWFLAKE_INSECURE_MODE\",  \"True\").lower() == \"true\",\n",
    "}\n",
    "\n",
    "# Step 1: Diagnose and fix any CSV issues BEFORE staging\n",
    "print(\"\\n1. Diagnosing and fixing CSV issues...\")\n",
    "csv_stats = diagnose_and_fix_csv_issues(DATA_DIR, schema_meta)\n",
    "\n",
    "# Step 2: ONLY AFTER verification, stage the fixed CSV files to Snowflake\n",
    "print(\"\\n2. Staging verified CSVs to Snowflake...\")\n",
    "conn = snowflake.connector.connect(**SF_CONN)\n",
    "cur = conn.cursor()\n",
    "for path in DATA_DIR.glob(\"*.csv\"):\n",
    "    uri = f\"file:///{path.resolve().as_posix()}\"\n",
    "    print(f\" • PUT {path.name}\")\n",
    "    cur.execute(f\"PUT '{uri}' @~/ OVERWRITE=TRUE\")\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Step 3: Prepare Snowflake environment\n",
    "print(\"\\n3. Preparing Snowflake environment...\")\n",
    "sess = Session.builder.configs(SF_CONN).create()\n",
    "sess.use_warehouse(SF_CONN[\"warehouse\"])\n",
    "sess.use_database(SF_CONN[\"database\"])\n",
    "sess.use_schema(SF_CONN[\"schema\"])\n",
    "\n",
    "# Get a reader with FIXED options for the quote/escape conflict\n",
    "# NOTE: Modified to avoid the ESCAPE parameter that's causing conflicts\n",
    "print(\"\\n3a. Creating Snowflake reader with compatible quote settings...\")\n",
    "reader = sess.read \\\n",
    "    .option(\"parse_header\", True) \\\n",
    "    .option(\"field_delimiter\", \",\") \\\n",
    "    .option(\"field_optionally_enclosed_by\", '\"')  # Use this instead of quote/escape combination\n",
    "\n",
    "# Drop existing tables if needed\n",
    "print(\"\\n3b. Dropping existing tables...\")\n",
    "for tbl_name in schema_meta[\"tables\"].keys():\n",
    "    table_ident = f'\"{tbl_name.upper()}\"'\n",
    "    try:\n",
    "        sess.sql(f\"DROP TABLE IF EXISTS {table_ident}\").collect()\n",
    "        print(f\"  • Dropped {table_ident}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  • Failed to drop {table_ident}: {str(e)}\")\n",
    "\n",
    "# Helper function to convert schema types\n",
    "def to_snow_type(tstr):\n",
    "    t = str(tstr).upper()\n",
    "    if t.startswith(\"INT\"):    \n",
    "        return IntegerType()\n",
    "    if t.startswith((\"FLOAT\",\"DECIMAL\",\"NUMERIC\",\"DOUBLE\")): \n",
    "        return FloatType()\n",
    "    if t.startswith(\"DATE\"):   \n",
    "        return DateType()\n",
    "    return StringType()\n",
    "\n",
    "# Step 4: Load tables into Snowflake\n",
    "print(\"\\n4. Loading tables into Snowflake...\")\n",
    "loaded_tables = []\n",
    "failed_tables = []\n",
    "\n",
    "for tbl_name, tbl_info in schema_meta[\"tables\"].items():\n",
    "    # Build schema from metadata\n",
    "    fields = []\n",
    "    for col_name, col_meta in tbl_info.get(\"columns\", {}).items():\n",
    "        if isinstance(col_meta, dict):\n",
    "            raw_t = col_meta.get(\"type\", \"string\")\n",
    "        else:\n",
    "            raw_t = str(col_meta)\n",
    "        fields.append(StructField(col_name, to_snow_type(raw_t)))\n",
    "    \n",
    "    schema = StructType(fields)\n",
    "    table_ident = f'\"{tbl_name.upper()}\"'\n",
    "    \n",
    "    try:\n",
    "        # Load the table with compatible reader options\n",
    "        print(f\" • {table_ident:30s}\", end=\" \")\n",
    "        df_snow = reader.schema(schema).csv(f\"@~/{tbl_name}.csv\")\n",
    "        df_snow.write.mode(\"overwrite\").save_as_table(table_ident)\n",
    "        print(\"✓\")\n",
    "        loaded_tables.append(tbl_name)\n",
    "    except Exception as e:\n",
    "        print(\"✗\")\n",
    "        print(f\"   Error: {str(e)}\")\n",
    "        \n",
    "        # Try with alternate options if the first attempt fails\n",
    "        try:\n",
    "            print(f\"   Trying alternate loading approach...\")\n",
    "            # Create a different reader with simpler options\n",
    "            alt_reader = sess.read \\\n",
    "                .option(\"parse_header\", True) \\\n",
    "                .option(\"skip_blank_lines\", True)\n",
    "                \n",
    "            df_snow = alt_reader.schema(schema).csv(f\"@~/{tbl_name}.csv\")\n",
    "            print(f\" • RETRY {table_ident:30s}\", end=\" \")\n",
    "            df_snow.write.mode(\"overwrite\").save_as_table(table_ident)\n",
    "            print(\"✓\")\n",
    "            loaded_tables.append(tbl_name)\n",
    "        except Exception as retry_error:\n",
    "            print(\"✗\")\n",
    "            print(f\"   Retry failed: {str(retry_error)}\")\n",
    "            failed_tables.append(tbl_name)\n",
    "\n",
    "# Close the Snowflake session\n",
    "sess.close()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n===== Snowflake Load Summary =====\")\n",
    "print(f\"Total tables processed: {len(loaded_tables) + len(failed_tables)}\")\n",
    "print(f\"Successfully loaded: {len(loaded_tables)}\")\n",
    "if failed_tables:\n",
    "    print(f\"Failed to load: {len(failed_tables)}\")\n",
    "    print(\" • \" + \"\\n • \".join(failed_tables))\n",
    "print(\"===================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthetic_data_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

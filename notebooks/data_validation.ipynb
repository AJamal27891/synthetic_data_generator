{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Validation & Snowflake Load  \n",
    "This notebook validates the synthetic TPC-DS–like dataset generated by our project.  It performs the following checks:  \n",
    "- Ensures all CSVs defined in the lineage config are present and loaded  \n",
    "- Verifies primary key uniqueness for each table  \n",
    "- Checks foreign key consistency (normalizing any `_date` columns)  \n",
    "- Stages the files to Snowflake  \n",
    "- Loads each table into Snowflake using `schema_metadata.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load environment variables, configurations, and build dynamic maps  \n",
    "In this cell we read Snowflake credentials from `.env`, load our table/lineage YAML, and derive the list of CSVs, date columns, PKs, and FK rules entirely from config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Will load CSVs: ['customers.csv', 'dates.csv', 'employees.csv', 'ghost.csv', 'inventory.csv', 'legacy_customers.csv', 'orders.csv', 'orders_backup.csv', 'products.csv', 'promotions.csv', 'returns.csv', 'sales_targets.csv', 'stores.csv', 'suppliers.csv', 'transactions.csv', 'transactions_backup.csv']\n",
      "→ Date parsing map: {'promotions': ['start_date', 'end_date'], 'dates': ['full_date', 'day', 'month', 'year', 'weekday'], 'employees': ['hire_date'], 'orders': ['order_date'], 'inventory': ['inventory_date'], 'returns': ['return_date']}\n",
      "→ Primary keys: {'customers': 'customer_id', 'products': 'product_id', 'stores': 'store_id', 'promotions': 'promo_id', 'dates': 'date_id', 'suppliers': 'supplier_id', 'employees': 'employee_id', 'orders': 'order_id', 'transactions': 'transaction_id', 'returns': 'return_id'}\n",
      "→ Foreign key rules: [{'table': 'orders', 'column': 'customer_id', 'references': 'customers.customer_id'}, {'table': 'orders', 'column': 'store_id', 'references': 'stores.store_id'}, {'table': 'transactions', 'column': 'order_id', 'references': 'orders.order_id'}, {'table': 'transactions', 'column': 'product_id', 'references': 'products.product_id'}, {'table': 'transactions', 'column': 'supplier_id', 'references': 'suppliers.supplier_id'}, {'table': 'inventory', 'column': 'store_id', 'references': 'stores.store_id'}, {'table': 'inventory', 'column': 'product_id', 'references': 'products.product_id'}, {'table': 'returns', 'column': 'order_id', 'references': 'orders.order_id'}, {'table': 'returns', 'column': 'product_id', 'references': 'products.product_id'}, {'table': 'sales_targets', 'column': 'store_id', 'references': 'stores.store_id'}, {'table': 'employees', 'column': 'store_id', 'references': 'stores.store_id'}, {'table': 'orders', 'column': 'order_date', 'references': 'dates.full_date'}, {'table': 'inventory', 'column': 'inventory_date', 'references': 'dates.full_date'}, {'table': 'returns', 'column': 'return_date', 'references': 'dates.full_date'}, {'table': 'sales_targets', 'column': 'month', 'references': 'dates.month'}, {'table': 'sales_targets', 'column': 'year', 'references': 'dates.year'}]\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "import os, json, yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ——————————————————————————————\n",
    "# Setup paths & env\n",
    "# ——————————————————————————————\n",
    "ROOT_DIR = Path.cwd().parent             # one level up from notebooks/\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "META_DIR = ROOT_DIR / \"metadata\"\n",
    "CFG_DIR  = ROOT_DIR / \"data_generation_config\"\n",
    "load_dotenv(dotenv_path=ROOT_DIR / \".env\")\n",
    "\n",
    "# ——————————————————————————————\n",
    "# Load YAML configs\n",
    "# ——————————————————————————————\n",
    "with open(CFG_DIR / \"tables.yml\")  as f: tbl_cfg = yaml.safe_load(f)\n",
    "with open(CFG_DIR / \"lineage.yml\") as f: lin_cfg = yaml.safe_load(f)\n",
    "\n",
    "# ——————————————————————————————\n",
    "# 1.a) Build list of CSVs from lineage steps\n",
    "# ——————————————————————————————\n",
    "file_set = set()\n",
    "for step in lin_cfg[\"steps\"]:\n",
    "    file_set.update(step.get(\"inputs\", []))\n",
    "    file_set.update(step.get(\"outputs\", []))\n",
    "# include any backups if present on disk\n",
    "for p in DATA_DIR.glob(\"*_backup.csv\"):\n",
    "    file_set.add(p.name)\n",
    "file_list = sorted(file_set)\n",
    "\n",
    "# ——————————————————————————————\n",
    "# 1.b) Derive date columns from tables.yml\n",
    "# ——————————————————————————————\n",
    "date_map = {}\n",
    "from datetime import timedelta, datetime\n",
    "for tname, tdef in tbl_cfg[\"tables\"].items():\n",
    "    dates = []\n",
    "    for col, cdef in tdef[\"columns\"].items():\n",
    "        # faker.date_between or derived expr containing 'date'\n",
    "        if (cdef[\"type\"] == \"faker\"       and cdef.get(\"method\")==\"date_between\") \\\n",
    "        or (cdef[\"type\"] == \"derived\"     and \"date\" in cdef.get(\"expr\",\"\")):\n",
    "            dates.append(col)\n",
    "    if dates:\n",
    "        date_map[tname] = dates\n",
    "\n",
    "# 1.c) Derive PKs by convention: first int _id\n",
    "PKS = {}\n",
    "for tname, tdef in tbl_cfg[\"tables\"].items():\n",
    "    for col, cdef in tdef[\"columns\"].items():\n",
    "        if col.endswith(\"_id\") and cdef[\"type\"]==\"int\":\n",
    "            PKS[tname] = col\n",
    "            break\n",
    "\n",
    "# 1.d) Load schema_metadata for FK definitions\n",
    "schema_meta   = json.load(open(META_DIR/\"schema_metadata.json\"))\n",
    "relationships = schema_meta.get(\"relationships\", [])\n",
    "\n",
    "print(\"→ Will load CSVs:\", file_list)\n",
    "print(\"→ Date parsing map:\", date_map)\n",
    "print(\"→ Primary keys:\", PKS)\n",
    "print(\"→ Foreign key rules:\", relationships)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load CSVs with proper date parsing  \n",
    "Here we load each CSV present on disk, automatically parsing only those columns whose names end in `_date`, and skip any files declared in lineage but not generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSVs…\n",
      " • customers            → 10000 rows ×  8 cols\n",
      " • dates                →   365 rows ×  6 cols  (dates: ['full_date'])\n",
      " • employees            →   200 rows ×  5 cols  (dates: ['hire_date'])\n",
      " • ghost                MISSING on disk, skipping\n",
      " • inventory            → 10000 rows ×  4 cols  (dates: ['inventory_date'])\n",
      " • legacy_customers     →   500 rows ×  9 cols\n",
      " • orders               → 10000 rows ×  6 cols  (dates: ['order_date'])\n",
      " • orders_backup        → 10000 rows ×  6 cols  (dates: ['order_date'])\n",
      " • products             →  1000 rows ×  6 cols\n",
      " • promotions           →   100 rows ×  5 cols  (dates: ['start_date', 'end_date'])\n",
      " • returns              →  1000 rows ×  5 cols  (dates: ['return_date'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27788\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27788\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27788\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27788\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27788\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27788\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27788\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27788\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27788\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27788\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27788\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " • sales_targets        →  1200 rows ×  4 cols\n",
      " • stores               →   100 rows ×  5 cols\n",
      " • suppliers            →    50 rows ×  3 cols\n",
      " • transactions         → 30000 rows ×  7 cols\n",
      " • transactions_backup  → 30095 rows ×  7 cols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27788\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27788\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27788\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27788\\2800352304.py:21: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# 2) Load CSVs with _only_ real date columns\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "dfs = {}\n",
    "print(\"Loading CSVs…\")\n",
    "for fname in file_list:\n",
    "    path  = DATA_DIR / fname\n",
    "    table = path.stem\n",
    "\n",
    "    if not path.exists():\n",
    "        print(f\" • {table:20s} MISSING on disk, skipping\")\n",
    "        continue\n",
    "\n",
    "    # Peek at columns\n",
    "    sample = pd.read_csv(path, nrows=0)\n",
    "    # Only parse those ending in '_date'\n",
    "    parse_dt = [c for c in sample.columns if c.lower().endswith(\"_date\")]\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        parse_dates=parse_dt,\n",
    "        infer_datetime_format=True\n",
    "    )\n",
    "    dfs[table] = df\n",
    "    print(f\" • {table:20s} → {df.shape[0]:5d} rows × {df.shape[1]:2d} cols\"\n",
    "          + (f\"  (dates: {parse_dt})\" if parse_dt else \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify primary key uniqueness for each table  \n",
    "We infer the primary key for each table by convention (`*_id`) and check that every value is unique, reporting PASS/FAIL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primary Key Checks:\n",
      " • customers            10000/10000 → PASS\n",
      " • products              1000/1000  → PASS\n",
      " • stores                 100/100   → PASS\n",
      " • promotions             100/100   → PASS\n",
      " • dates                  365/365   → PASS\n",
      " • suppliers               50/50    → PASS\n",
      " • employees              200/200   → PASS\n",
      " • orders               10000/10000 → PASS\n",
      " • transactions         30000/30000 → PASS\n",
      " • returns               1000/1000  → PASS\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "print(\"\\nPrimary Key Checks:\")\n",
    "for tbl, pk in PKS.items():\n",
    "    df = dfs.get(tbl)\n",
    "    if df is None: \n",
    "        print(f\" • {tbl:20s} MISSING\") \n",
    "        continue\n",
    "    unique = df[pk].nunique()\n",
    "    total  = len(df)\n",
    "    print(f\" • {tbl:20s} {unique:5d}/{total:<5d} →\"\n",
    "          + (\" PASS\" if unique==total else \" FAIL\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check foreign key consistency (with normalized dates)  \n",
    "All `_date` columns are normalized to midnight, then we join each child→parent FK and count any mismatches, reporting PASS or the number of failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Foreign Key Checks:\n",
      " • orders         .customer_id     → customers      .customer_id     : PASS\n",
      " • orders         .store_id        → stores         .store_id        : PASS\n",
      " • transactions   .order_id        → orders         .order_id        : PASS\n",
      " • transactions   .product_id      → products       .product_id      : PASS\n",
      " • transactions   .supplier_id     → suppliers      .supplier_id     : PASS\n",
      " • inventory      .store_id        → stores         .store_id        : PASS\n",
      " • inventory      .product_id      → products       .product_id      : PASS\n",
      " • returns        .order_id        → orders         .order_id        : PASS\n",
      " • returns        .product_id      → products       .product_id      : PASS\n",
      " • sales_targets  .store_id        → stores         .store_id        : PASS\n",
      " • employees      .store_id        → stores         .store_id        : PASS\n",
      " • orders         .order_date      → dates          .full_date       : FAIL (29)\n",
      " • inventory      .inventory_date  → dates          .full_date       : PASS\n",
      " • returns        .return_date     → dates          .full_date       : PASS\n",
      " • sales_targets  .month           → dates          .month           : PASS\n",
      " • sales_targets  .year            → dates          .year            : PASS\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# 3) Foreign Key Consistency with normalized dates\n",
    "import numpy as np\n",
    "\n",
    "# Normalize any datetime columns ending in '_date'\n",
    "for tbl, df in dfs.items():\n",
    "    for col in df.columns:\n",
    "        if col.lower().endswith(\"_date\") and np.issubdtype(df[col].dtype, np.datetime64):\n",
    "            df[col] = df[col].dt.normalize()\n",
    "\n",
    "def fk_issues(child_df, ckey, parent_df, pkey):\n",
    "    return (~child_df[ckey].dropna().isin(parent_df[pkey].dropna())).sum()\n",
    "\n",
    "print(\"\\nForeign Key Checks:\")\n",
    "for rel in relationships:\n",
    "    child, ckey = rel[\"table\"], rel[\"column\"]\n",
    "    parent, pkey = rel[\"references\"].split(\".\",1)\n",
    "    df_c = dfs.get(child)\n",
    "    df_p = dfs.get(parent)\n",
    "    if df_c is None or df_p is None:\n",
    "        print(f\" • {child}.{ckey} → {parent}.{pkey}: TABLE_MISSING\")\n",
    "        continue\n",
    "    issues = fk_issues(df_c, ckey, df_p, pkey)\n",
    "    status = \"PASS\" if issues == 0 else f\"FAIL ({issues})\"\n",
    "    print(f\" • {child:15s}.{ckey:15s} → {parent:15s}.{pkey:15s} : {status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage CSVs and load tables into Snowflake  \n",
    "We PUT each CSV into the user stage, then use Snowpark (with `parse_header` option) and our `schema_metadata.json` to CREATE/OVERWRITE each table in Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Staging CSVs to Snowflake…\n",
      " PUT file:///C:/Users/user/Desktop/job_assignments/Thesis work/Thesis_work_old/synthetic_data_generator/data/customers.csv\n",
      " PUT file:///C:/Users/user/Desktop/job_assignments/Thesis work/Thesis_work_old/synthetic_data_generator/data/dates.csv\n",
      " PUT file:///C:/Users/user/Desktop/job_assignments/Thesis work/Thesis_work_old/synthetic_data_generator/data/employees.csv\n",
      " PUT file:///C:/Users/user/Desktop/job_assignments/Thesis work/Thesis_work_old/synthetic_data_generator/data/inventory.csv\n",
      " PUT file:///C:/Users/user/Desktop/job_assignments/Thesis work/Thesis_work_old/synthetic_data_generator/data/legacy_customers.csv\n",
      " PUT file:///C:/Users/user/Desktop/job_assignments/Thesis work/Thesis_work_old/synthetic_data_generator/data/orders.csv\n",
      " PUT file:///C:/Users/user/Desktop/job_assignments/Thesis work/Thesis_work_old/synthetic_data_generator/data/orders_backup.csv\n",
      " PUT file:///C:/Users/user/Desktop/job_assignments/Thesis work/Thesis_work_old/synthetic_data_generator/data/products.csv\n",
      " PUT file:///C:/Users/user/Desktop/job_assignments/Thesis work/Thesis_work_old/synthetic_data_generator/data/promotions.csv\n",
      " PUT file:///C:/Users/user/Desktop/job_assignments/Thesis work/Thesis_work_old/synthetic_data_generator/data/returns.csv\n",
      " PUT file:///C:/Users/user/Desktop/job_assignments/Thesis work/Thesis_work_old/synthetic_data_generator/data/sales_targets.csv\n",
      " PUT file:///C:/Users/user/Desktop/job_assignments/Thesis work/Thesis_work_old/synthetic_data_generator/data/stores.csv\n",
      " PUT file:///C:/Users/user/Desktop/job_assignments/Thesis work/Thesis_work_old/synthetic_data_generator/data/suppliers.csv\n",
      " PUT file:///C:/Users/user/Desktop/job_assignments/Thesis work/Thesis_work_old/synthetic_data_generator/data/transactions.csv\n",
      " PUT file:///C:/Users/user/Desktop/job_assignments/Thesis work/Thesis_work_old/synthetic_data_generator/data/transactions_backup.csv\n",
      "\n",
      "Loading into Snowflake tables…\n",
      " • \"CUSTOMERS\"                    ✔\n",
      " • \"PRODUCTS\"                     ✔\n",
      " • \"STORES\"                       ✔\n",
      " • \"PROMOTIONS\"                   ✔\n",
      " • \"DATES\"                        ✔\n",
      " • \"SUPPLIERS\"                    ✔\n",
      " • \"EMPLOYEES\"                    ✔\n",
      " • \"ORDERS\"                       ✔\n",
      " • \"TRANSACTIONS\"                 ✔\n",
      " • \"INVENTORY\"                    ✔\n",
      " • \"RETURNS\"                      ✔\n",
      " • \"SALES_TARGETS\"                ✔\n",
      " • \"LEGACY_CUSTOMERS\"             ✔\n",
      "✅ Snowflake load complete.\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# 5) Staging & Loading to Snowflake\n",
    "\n",
    "import snowflake.connector\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.types import StructType, StructField, IntegerType, FloatType, StringType, DateType\n",
    "\n",
    "# Stage CSVs to user stage (~)\n",
    "print(\"\\nStaging CSVs to Snowflake…\")\n",
    "conn = snowflake.connector.connect(**SF_CONN)\n",
    "cur = conn.cursor()\n",
    "for path in DATA_DIR.glob(\"*.csv\"):\n",
    "    uri = f\"file:///{path.resolve().as_posix()}\"\n",
    "    print(\" PUT\", uri)\n",
    "    cur.execute(f\"PUT '{uri}' @~/ OVERWRITE=TRUE\")\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Load into Snowflake tables\n",
    "print(\"\\nLoading into Snowflake tables…\")\n",
    "sess = Session.builder.configs(SF_CONN).create()\n",
    "sess.use_warehouse(SF_CONN[\"warehouse\"])\n",
    "sess.use_database(SF_CONN[\"database\"])\n",
    "sess.use_schema(SF_CONN[\"schema\"])\n",
    "\n",
    "def to_snow_type(tstr):\n",
    "    t = tstr.upper()\n",
    "    if t.startswith(\"INT\"):    return IntegerType()\n",
    "    if t.startswith((\"FLOAT\",\"DECIMAL\",\"NUMERIC\",\"DOUBLE\")): return FloatType()\n",
    "    if t.startswith(\"DATE\"):   return DateType()\n",
    "    return StringType()\n",
    "\n",
    "# Use the proper 'parse_header' option\n",
    "reader = sess.read.option(\"parse_header\", True).option(\"field_delimiter\", \",\")\n",
    "\n",
    "for tbl_name, tbl_info in schema_meta[\"tables\"].items():\n",
    "    # Build schema from metadata\n",
    "    fields = []\n",
    "    for col_name, col_meta in tbl_info[\"columns\"].items():\n",
    "        raw_t = col_meta[\"type\"] if isinstance(col_meta, dict) else str(col_meta)\n",
    "        fields.append(StructField(col_name, to_snow_type(raw_t)))\n",
    "    schema = StructType(fields)\n",
    "\n",
    "    df_snow = reader.schema(schema).csv(f\"@~/{tbl_name}.csv\")\n",
    "    table_ident = f'\"{tbl_name.upper()}\"'\n",
    "    print(f\" • {table_ident:30s}\", end=\" \")\n",
    "    df_snow.write.mode(\"overwrite\").save_as_table(table_ident)\n",
    "    print(\"✔\")\n",
    "\n",
    "sess.close()\n",
    "print(\"✅ Snowflake load complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthetic_data_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
